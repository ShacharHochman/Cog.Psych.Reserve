[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cognitive Psychology Reserve",
    "section": "",
    "text": "The Dot-Probe Task is Probably Fine\n\n\nBayesian modeling of reliability with brms\n\n\n\n\n\n\n\n\nMar 1, 2025\n\n\nShachar Hochman\n\n\n\n\n\n\n\n\n\n\n\n\nWho am I\n\n\n\n\n\nFrom cognitive science to data insights\n\n\n\n\n\nFeb 25, 2025\n\n\nShachar Hochman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Who.Am.I/index.html",
    "href": "posts/Who.Am.I/index.html",
    "title": "Who am I",
    "section": "",
    "text": "Hey! I am Shachar!\nAfter completing my PhD in cognitive psychology (HUJI & BGU) and postdoc at the University of Surrey in 2024, I’ve stepped beyond the boundaries of academic mind research. I’m now applying the complex behavioral modeling, psychometrics, and advanced statistical techniques that I developed studying cognition in new domains.\nThis blog is where I’ll revisit thought-provoking ideas from my cognitive psychology days and also demonstrate how similar approaches can reveal unexpected connections in entirely different fields.\n\nFeel free to connect with me if you’re interested in discussing these topics further!"
  },
  {
    "objectID": "posts/DotProbeReliability/index.html",
    "href": "posts/DotProbeReliability/index.html",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "",
    "text": "Goal: Discuss two methods to detect reliability (of cognitive tasks in this case) using Bayesian hierarchical linear models (HLMs) in brms.\nCase Study: The Dot-Probe Task. Contrary to a recent publication that claims the emotional dot-probe task is not a reliable measure, I’ll show that more nuanced modeling suggests the task is probably “fine” reliability-wise.\nThe Bottom Line: This tutorial-like post demonstrates how Bayesian HLM can fundamentally change reliability assessment. By analyzing trial-level data rather than aggregated scores, these methods can uncover meaningful signal where traditional approaches see only noise—potentially rehabilitating numerous tasks previously deemed psychometrically inadequate.\n\n\n\n\n\n\n\nI make assumptions (too)!\n\n\n\nI assume you:\n\nare vaguely familiar with hierarchical linear models (HLM) and Bayesian statistics 1.\nknow basic concepts in psychometrics - mainly the idea of “reliability”.\n\nThis post uses R with a tidyverse approach. My goal is to show how others’ great theoretical works can be applied in brms. If you’re already a fan, welcome. If not, get ready to fall in love!"
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#why-are-we-here",
    "href": "posts/DotProbeReliability/index.html#why-are-we-here",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "",
    "text": "Goal: Discuss two methods to detect reliability (of cognitive tasks in this case) using Bayesian hierarchical linear models (HLMs) in brms.\nCase Study: The Dot-Probe Task. Contrary to a recent publication that claims the emotional dot-probe task is not a reliable measure, I’ll show that more nuanced modeling suggests the task is probably “fine” reliability-wise.\nThe Bottom Line: This tutorial-like post demonstrates how Bayesian HLM can fundamentally change reliability assessment. By analyzing trial-level data rather than aggregated scores, these methods can uncover meaningful signal where traditional approaches see only noise—potentially rehabilitating numerous tasks previously deemed psychometrically inadequate.\n\n\n\n\n\n\n\nI make assumptions (too)!\n\n\n\nI assume you:\n\nare vaguely familiar with hierarchical linear models (HLM) and Bayesian statistics 1.\nknow basic concepts in psychometrics - mainly the idea of “reliability”.\n\nThis post uses R with a tidyverse approach. My goal is to show how others’ great theoretical works can be applied in brms. If you’re already a fan, welcome. If not, get ready to fall in love!"
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#a-blessing-and-a-curse---the-reliability-paradox",
    "href": "posts/DotProbeReliability/index.html#a-blessing-and-a-curse---the-reliability-paradox",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "A Blessing and a Curse - the “Reliability Paradox”",
    "text": "A Blessing and a Curse - the “Reliability Paradox”\nScientists love finding significant results. Cognitive psychologists, in particular, aim to build “robust tasks”—ones that consistently produce similar significant effects when averaged across participants. When participants perform more similarly (showing less between-subject variance), effects tend to be more stable and therefore significant, leading to happy researchers. But here’s the ironic twist: to get that consistent effect, tasks often get designed or refined until nearly all participants show the same pattern. In other words, cognitive psychologists (perhaps inadvertently) end up with tasks that minimize individual differences.\nThis is a thorny situation that reveals itself when researchers try to correlate their robust tasks performance with other measures of individual differences like questionnaires, or performance on other tasks. That’s when they discover there simply isn’t enough variability between participants to work with and to differentiate between their subjects.\nThe complexity worsens when the sought-after effect is a difference between two conditions of the same task (i.e., a within-subject condition). Take, for instance, when we measure attentional bias by comparing how quickly people respond to a probe that appears behind threatening versus neutral images—in this case researchers subtract one reaction time (RT; the score from the threatening stimuli) from another (the score from the neutral stimuli) in what’s known as the dot-probe task. This introduces the “reliability of difference scores” problem into play—a well-documented challenge in measuring and understanding change. For both theoretical and statistical reasons, subtracting two within-subjects conditions produces individual difference scores with poor psychometric properties. These scores typically show weak correlations with themselves both when taken across time points (i.e., low test-retest reliability) and within a single administration (i.e., low internal reliability). And here is where the thorn hits again, because if these measures can’t correlate with themselves, how (in the hell) can they correlate with anything else?\nThis tension between “robust tasks” (good for group-level effects) and high reliability (good for individual differences) is the heart of the so-called “reliability paradox,” as popularized by Hedge et al. and Draheim et al.. These well-rounded and accessible papers explore this paradox in depth, making further elaboration here unnecessary. The key insight here is that, there is an inherent pitfall in the meeting point of the endeavor after robust tasks, difference scores and individual differences."
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#on-repelling-the-reliability-curse",
    "href": "posts/DotProbeReliability/index.html#on-repelling-the-reliability-curse",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "On repelling the reliability curse",
    "text": "On repelling the reliability curse\nThere are generally two approaches to addressing the reliability paradox:\n\nBetter task design – creating more challenging tasks or developing multiple ways to tap into the construct of interest, thereby generating more within-subjects variation. This direction has already yielded promising results (for example).\nNuanced Statistical modeling - analyzing trial-by-trial data with a model (often Bayesian HLM) that explicitly teases out and retains individual differences.\n\n\nOn Bayesian Hierarchical Models\nI won’t re-argue the entire case for Bayesian HLMs here (see footnote 1, these great materials, and don’t miss Nathaniel Haines blog). Suffice it to say that:\n\nTrial-Level modeling: Bayesian HLMs are generative models that we will feed trial-by-trial data. This means our models produce informed distributions for each participant in each condition. By considering trial-by-trial variation through this model-informed approach, we get nuanced measures of individual performance—precious information that would be lost in traditional ANOVA-style aggregation.\nDistributional Flexibility: Traditional linear models or ANOVAs assume our measures ultimately follow a Gaussian distribution. With Bayesian HLMs, we can choose distributions that actually match our data. Take RTs, for instance—they’re right-skewed and always positive. We can model this reality using a lognormal distribution (more on this later), allowing individual distributions to follow suit. This capability is particularly valuable for individual differences research.\nPosterior Distribution: the Bayesian philosophy itself offers a distinct advantage. Instead of getting a single reliability value, we get posterior distributions—entire spectrums of possible values. This provides a richer, more nuanced perspective on reliability in our data."
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#the-case-study-revisiting-the-dot-probe-task",
    "href": "posts/DotProbeReliability/index.html#the-case-study-revisiting-the-dot-probe-task",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "The Case Study: Revisiting the Dot-Probe Task",
    "text": "The Case Study: Revisiting the Dot-Probe Task\nXu et al. (2024) have recently provided a compelling case study. They set out to test whether the “emotional dot‐probe task”—a long-standing paradigm in cognitive psychology—can truly capture an attentional bias to threat with any consistency. In a typical trial of the dot-probe task, two images (one threatening, one neutral) are flashed on opposite sides of the screen. After a brief interval, these images disappear and a target (often a letter like “E” or “F”) appears in the location of one of the images. The idea is that if you’re quicker to respond when the target replaces the threat, you’ve got a bias toward threat. Xu et al. took this classic design further by testing 36 variations of the task—tweaking everything from stimulus type (faces, scenes, snakes/spiders) to stimulus orientation (horizontal vs. vertical) and the timing of the stimulus onset asynchronies (SOAs; 100, 500, or 900 ms). Their stark finding: almost all variations of the task produced internal reliability estimates that were essentially zero, suggesting that this task holds little promise for detecting individual differences.\nIn this post, I reanalyze a subset of Xu et al.’s data with Bayesian HLM, hypothesizing that a more sophisticated approach might rescue some of that lost individual variance.\n\n\n\n\n\n\nPre-processing the Data\n\n\n\n\n\nI took the large datasets from Xu et al. and made several key decisions:\n\nFocusing on Study 1: This study closely examined the reliability of the emotional dot‐probe task, making it ideal for reanalysis.\nSelecting Task Variants: To ensure a substantial sample size, I combined data from six variants of the task (out of the 36 tested in the paper). All used faces as threatening stimuli on vertical display, differing in their SOAs and whether additional neutral trials were included. Only threat-congruent (TC) and threat-incongruent (TIC) trials were analyzed.\nData Cleaning and Filtering: Building on Xu et al.’s exclusion criteria—participants with less than 60% accuracy or median RTs under 300 ms—I added a few more:\n\n\nIncluded only mouse responses to reduce device-related variability.\nRetained only correct response.\nExcluded responses faster than 250 ms (likely anticipatory) and slower than 3500 ms (potential distractions).\nOutliers control: I removed trials where RTs deviated more than 3.29 median absolute deviations from the median per subject and condition.\n\nThese are quite conservative steps. While many may not be strictly necessary for individual differences studies, they are standard in classic cognitive task analyses. Our total sample size now is 698 participants (!).\nHere is the R code for the preprocessing:\n\nlibrary(tidyverse)\n\n`%ni%` &lt;- Negate(`%in%`)\n\nStudy_1 &lt;- readr::read_csv(\"openData_study1_trials_102823.csv\")\ns1.all.outcomes &lt;- readr::read_csv(\"openData_study1_outcomes_102823.csv\")\n\n# Exclude participants with &lt;60% accuracy or median RT &lt;300 ms\ns1.excluded.id &lt;- s1.all.outcomes %&gt;%\n  filter(all_accuracy &lt; 0.6 | all_medRTc &lt; 300) %&gt;%\n  pull(id)\n\n\nStudy1_fs &lt;- Study_1 %&gt;%\n  filter(id %ni% s1.excluded.id, \n         condition != \"practice\", \n         resp_type != \"timeout\") %&gt;%\n  filter(test_id %in% c(1:3, 19:21), \n         resp_type == \"mouse\") %&gt;% \n  mutate(\n    RT = as.numeric(rt) / 1000,  # Convert RT to seconds\n    accuracy = as.numeric(correct),\n    trial.all.type = row_number(), \n    split = as.factor(ifelse(trial.all.type %% 2 == 1, 1, 2)), \n    condition = case_when(\n      condition == 1 ~ \"TC\",\n      condition == 2 ~ \"TIC\",\n      condition == 3 ~ \"TT\",\n      condition == 4 ~ \"NN\"\n    )\n   ) %&gt;%\n   filter(RT &gt; 0.25, RT &lt; 3.5, accuracy == 1, condition %in% c(\"TC\", \"TIC\")) %&gt;%\n   group_by(id, condition) %&gt;%\n   mutate(Outlier = c(datawizard::standardize(RT, robust = TRUE))) %&gt;%\n   ungroup() %&gt;% \n   filter(abs(Outlier) &lt; 3.29)"
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#rouder-haaf-model-in-brms",
    "href": "posts/DotProbeReliability/index.html#rouder-haaf-model-in-brms",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "Rouder-Haaf Model in brms",
    "text": "Rouder-Haaf Model in brms\nNow that we understand the conceptual framework, let’s see how to implement this in brms. The model specification needs to be careful and precise - we’re dealing with reaction time data that has specific characteristics, and we want to capture best the nature of our measurements:\nlibrary(brms)\nlibrary(bayestestR)\n\noptions(contrasts = c(\"contr.equalprior\", \"contr.poly\"))\n\npriors &lt;- c(\n  prior(exponential(1), class = \"sd\", group = \"id\"),\n  prior(exponential(1), class = \"sd\", group = \"id\", dpar = \"sigma\"),  \n  \n  \n  prior(normal(0, 1), class = \"b\", dpar = \"sigma\"),  \n  \n  # Fixed effects\n  prior(normal(0, 0.1), class = \"b\"))\nWe start by loading the necessary packages and setting proper sum to zero contrast coding. The contr.equalprior ensures unbiased Bayesian estimation when working with factors - unlike traditional effect or treatment coding which can lead to biased priors (see here for a detailed explanation).\nNext, we define relatively weakly informative priors for all parameters. The random-effects standard deviations (for both the mean and the residual variance) receive exponential(1), reflecting mild assumptions about individual variability. Meanwhile, the fixed effects for the mean RT are assigned a normal(0, 0.1) prior, implying modest expected effect sizes. Finally, we allow for some uncertainty in the residual standard deviation by placing a normal(0, 1) prior on any parameters governing sigma. These choices are conservative enough to encourage stable estimation, yet flexible enough to capture meaningful individual differences in response times.\nDotProbeModel.RouderHaafModel.RDS &lt;- brm(\n  formula = bf(\n    RT | trunc(lb = 0.25, ub = 3.5) ~ \n      condition * split +\n      (condition * split|p|id),\n    \n    sigma ~ \n      condition * split +\n      (condition * split|p|id)\n    ),\n  prior = priors,\n\n  family = lognormal(), data = Study1_fs,\n\n  iter = 4000, warmup = 2000,\n\n  chains = 4,  cores = 4,\n\n  threads = threading(2),\n\n  control = list(adapt_delta = 0.95,\n                 max_treedepth=12),\n\n  init = 0, backend = \"cmdstanr\")\nThe model formula contains several important features:\n\nTruncation: trunc(lb = 0.25, ub = 3.5) tells the model our response variable (RT) cannot fall below 0.25 seconds or exceed 3.5 seconds. This matches our preprocessing decisions and helps the model make more accurate predictions by respecting the actual bounds of our data.\nDistribution Choice: family = lognormal() is crucial. Reaction times follow a characteristic right-skewed distribution—they can’t be negative, tend to have a longer right tail, and the variability often increases with the mean. Importantly, the lognormal distribution captures these properties naturally by modeling RTs on the log scale, where effects are multiplicative (e.g., a 10% slowdown instead of ±X ms).\nThis multiplicative framework elegantly handles proportional relationships (e.g., a participant slowing by 10% in threat trials retains the same effect size whether their baseline is 500 ms or 1000 ms). In the same vein, reliability estimates derived from the log scale reflect proportional differences. Nevertheless, it is mostly common to report and use absolute differences in raw milliseconds. Later, we’ll reconcile this by converting our reliability estimates to the raw RT scale using posterior predictions—ensuring consistency with Xu et al.’s approach and broader psychometric conventions.\nThe Sigma: The sigma ~ condition * split + (condition * split | p | id) part of the formula recognizes that people aren’t just different in their overall consistency, but that their variability can also differ by condition and split. Imagine some participants who remain steady across all conditions, while others might fluctuate more in one condition than another. By letting each participant have their own “baseline consistency” as well as condition- and split-specific effects, we are not just acknowledging that people vary overall—we’re allowing these differences to manifest across different experimental factors, rather than forcing everyone into a single error structure.\nThe lognormal distribution already accounts for the fact that reaction times naturally “spread out” as they lengthen (e.g., a 3-second average often has bigger ups/downs than a 1-second average). Here, we go a step further: each person can have a distinct sigma profile for each combination of condition and split—capturing a richer picture of individual differences.\nCorrelated Individual Effects: The |p| syntax in both the main formula (condition * split |p|id) and the sigma formula(condition * split |p|id) allows the model to account for an important reality of reaction time data: participants with longer RTs typically also show larger variability in their responses. Instead of assuming independent effects, this structure lets the model estimate correlations between individual differences in mean RTs and response variability. By doing so, it refines individual estimates even further.\n\n\nExtracting the reliability\nJust before examining the reliability using a full Bayesian approach à la “Rouder and Haaf”, let’s look at what we get using the classic method for calculating internal consistency.\n\n\n\n\n\n\n\n\n\nThe results mirror those reported by Xu et al., showing a nearly zero correlation of -0.019 (and similarly poor results after applying the Spearman-Brown correction) – in fact, slightly negative in this case.\nNow for the main course: Rouder and Haaf reliability estimate. Using tidybayes, I extract the standard deviations of the random effects for both the congruity effect and the interaction and transforming them to the response scale.\nVar.df &lt;- DotProbeModel1 %&gt;%\n  spread_rvars(sd_id__trial_type1, `sd_id__trial_type1:half.all.type1`)\n  \nVar.df &lt;- Var.df %&gt;% \n  mutate(\n  sd_response_condition1_response = exp(sd_id__condition1^2 / 2) * sqrt(exp(sd_id__condition1^2) - 1),\n  sd_response_interaction_response = exp(`sd_id__condition1:split1`^2 / 2) * sqrt(exp(`sd_id__condition1:split1`^2) - 1)\n  )\nI square the standard deviations in order to get variance,\nVar.df_variance &lt;- Var.df %&gt;%\n  mutate(across(everything(), ~ .^2)) \nand then what I have left is to perform the subtraction and division (the rvars structure makes this remarkably straightforward):\nRouder.Haaf.numerator &lt;- \n  Var.df_variance$sd_id__trial_type1 - \n  Var.df_variance$`sd_id__trial_type1:half.all.type1`\n\nRouder.Haaf.denominator &lt;- \n  Var.df_variance$sd_id__trial_type1 + \n  Var.df_variance$`sd_id__trial_type1:half.all.type1`\n\nRouder.Haaf.Reliability &lt;- Rouder.Haaf.numerator/Rouder.Haaf.denominator\n\n\n\n\n\n\n\n\n\nThis is the posterior reliability coefficient distribution—a distribution of possible values for the reliability coefficient we are estimating, where the density reflects how likely each value is.\nWhat emerges here, is in my opinion, a fascinating picture of informative Bayesian complexity:\n\nFirst, we see the remarkable benefits of the Bayesian HLM approach, where 99.96% of the possible reliability values exceed the raw-aggregated value of -0.019. This dramatic difference highlights how traditional aggregation methods might severely underestimate the task’s reliability.\nAt the same time, the actual reliability of the Dot-probe task isn’t definitively established. The 95% credible interval suggests reliability values range from 0.43 to a practically perfect 1. While this wide posterior distribution indicates considerable uncertainty about the task’s reliability, it also allows us to make informed probability statements about specific thresholds. Based on the posterior distribution, there’s a 85.72% chance that the Dot-probe task reliability exceeds 0.6, a 77.65% chance it exceeds 0.7, and a 65.79% chance it exceeds 0.8. Such nuanced probabilistic conclusions aren’t possible with classic models and give researchers the tools to make reasoned decisions.\nBut what’s the most likely reliability? The Maximum A Posteriori probability estimate (MAP) point estimates follows for the most likely value and in this case indicates on practically perfect reliability of 0.998. More conservative estimates include the median at 0.892 and the mean at 0.823. All these point estimates suggest the task is substantially more reliable than previously thought, with even the most conservative estimate showing strong reliability.\n\nReminder: our example deals with “split-half” reliability, a type of “internal consistency”. It typically exceeds what you’d get with, for example, a test–retest scenario (different sessions, potential fatigue effects, etc.), so you might consider the values here as an upper bound on broader “reliability”."
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#implementing-chen-et-al.-in-brms",
    "href": "posts/DotProbeReliability/index.html#implementing-chen-et-al.-in-brms",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "Implementing Chen et al. in brms",
    "text": "Implementing Chen et al. in brms\nChen et al. take a distinct modeling approach by focusing on how condition effects (TC vs. TIC) vary across splits (odd vs. even trials). Their model omits both the global intercept and main effect of condition, instead modeling how these condition differences manifest through interactions with splits. This parameterization directly captures how stable individual differences in condition effects are across different portions of the task. Here’s how we implement this approach in brms. Rather than using a standard factorial structure (condition + split + condition:split), we separate the random effects into distinct components for splits and condition-by-split interactions:\nDotProbeModel.ChenModel &lt;- brm(\n  formula = bf(\n    RT | trunc(lb = 0.25, ub = 3.5) ~ 0 +\n      split+\n      condition:split+\n      (0+split|p|id)+\n      (0+split:condition|p|id),\n    \n    sigma ~ 0 +\n      split+\n      condition:split+\n      (0+split|p|id)+\n      (0+split:condition|p|id)\n  ),\n  prior = priors,\n\n  family = lognormal(), data = Study1_fs,\n\n  iter = 4000, warmup = 2000,\n\n  chains = 4,  cores = 4,\n\n  threads = threading(2),\n\n  control = list(adapt_delta = 0.95,\n                 max_treedepth=12),\n\n  init = 0, backend = \"cmdstanr\")\n\nExtracting the Reliability\nThe Chen et al. approach gives us a reliability estimate by correlating threat bias scores (TIC-TC) between splits. The process mirrors what we typically do in reliability analysis: we compute difference scores for each split level (odd vs even trials) and correlate them, but with an important Bayesian twist - we do this for every posterior draw from our model, giving us a full distribution of reliability estimates instead of just one number. I built on their approach by adding the Spearman-Brown correction - a standard adjustment in psychometrics that accounts for split-half calculations. We need this correction because we’re essentially working with half the data in each split, and it makes our estimates more comparable to full-test reliability. Plus, since Xu et al. used this correction in their original analysis, it lets us make direct comparisons.\n\n# Simulate expected RTs (milliseconds) for all splits/conditions\nnewdata &lt;- tidyr::expand_grid(\n  id = unique(Study1_fs$id),        \n  split = c(\"1\", \"2\"),         \n  condition = c(\"TIC\", \"TC\")          \n)\n\npred &lt;- add_epred_rvars(\n  DotProbeModel.ChenModel,\n  newdata = newdata,\n  re_formula = NULL           \n) %&gt;% \n  mutate(condition = interaction(split, condition, sep = \"_\")) \n\n# Compute threat bias (TIC - TC) for each split\ndiff_scores &lt;- pred %&gt;%\n  select(id, condition, .epred) %&gt;%\n  pivot_wider(names_from = condition, values_from = .epred) %&gt;%\n  mutate(\n    Effect_1 = `1_TIC` - `1_TC`,  # Split 1 effect (ms)\n    Effect_2 = `2_TIC` - `2_TC`   # Split 2 effect (ms)\n  ) \n\n# Calculate reliability per posterior draw\ncor_results &lt;- \n  data.frame(\n    correlation =  \n      with(diff_scores, {\n        rdo(\n          cor(Effect_1, Effect_2)\n          )})) %&gt;%\n  mutate(\n    spearman_brown = (2 * correlation) / (1 + correlation)\n    )\n\n\n\n\n\n\n\n\n\nThe posterior distributions reveal 95% credible intervals ranging from 0.41 to 0.81 for the raw correlation, and from 0.59 to 0.91 for the Spearman-Brown corrected reliability, with median estimates of 0.62 and 0.77, respectively. These values suggest moderate to strong reliability, though somewhat lower than the Rouder-Haaf estimates. This difference is instructive – while both approaches support the task’s reliability, they do so through different lenses. The Chen et al. method provides more conservative estimates that may feel more familiar to researchers used to traditional reliability metrics, while still demonstrating substantially higher reliability than previously reported using simpler methods."
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#footnotes",
    "href": "posts/DotProbeReliability/index.html#footnotes",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re wondering how to familiarize yourself with this material, I recommend starting with the excellent book “A Student’s Guide to Bayesian Statistics” by Ben Lambert, which offers a fantastic introduction, especially for those with some understanding of frequentist statistics. Now, I’m a spiral learner—I often need to revisit concepts from different perspectives. With that in mind, I suggest following up with “Statistical Rethinking” and/or “Doing Bayesian Data Analysis”, alongside the legendary bookdowns by Solomon Kurz — As the title of the callout probably reveals, I am a fan of his writings.↩︎"
  }
]