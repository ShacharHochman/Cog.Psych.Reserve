[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cognitive Psychology Reserve",
    "section": "",
    "text": "Beyond the Exclamation Points!!!\n\n\nHDI-ROPE for Binary Outcomes: What Makes a Text Message Spam?\n\n\n\n\n\n\n\n\nMay 21, 2025\n\n\nShachar Hochman\n\n\n\n\n\n\n\n\n\n\n\n\nThe Dot-Probe Task is Probably Fine\n\n\nBayesian modeling of reliability with brms\n\n\n\n\n\n\n\n\nMar 8, 2025\n\n\nShachar Hochman\n\n\n\n\n\n\n\n\n\n\n\n\nWho am I\n\n\n\n\n\nFrom cognitive science to data insights\n\n\n\n\n\nFeb 25, 2025\n\n\nShachar Hochman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Who.Am.I/index.html",
    "href": "posts/Who.Am.I/index.html",
    "title": "Who am I",
    "section": "",
    "text": "Hey! I am Shachar!\nAfter completing my PhD in cognitive psychology (HUJI & BGU) and postdoc at the University of Surrey in 2024, I’ve stepped beyond the boundaries of academic mind-brain-behavior research. I’m now applying the complex behavioral modeling, psychometrics, and advanced statistical techniques that I developed studying cognition in new domains.\nThis blog is where I’ll revisit thought-provoking ideas from my cognitive psychology days and also demonstrate how similar approaches can reveal unexpected connections in entirely different fields.\n\nFeel free to connect with me if you’re interested in discussing these topics further!"
  },
  {
    "objectID": "posts/DotProbeReliability/index.html",
    "href": "posts/DotProbeReliability/index.html",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "",
    "text": "Goal: Discuss two methods to detect reliability (of cognitive tasks in this case) using Bayesian hierarchical linear models (HLMs) in brms.\nCase Study: The Dot-Probe Task. Contrary to a recent publication that claims the emotional dot-probe task is not a reliable measure, I’ll show that more nuanced modeling suggests the task is probably “fine” reliability-wise.\nThe Bottom Line: This tutorial-like post demonstrates how Bayesian HLM can fundamentally change reliability assessment. By analyzing trial-level data rather than aggregated scores, these methods can uncover meaningful signal where traditional approaches see only noise—potentially rehabilitating numerous tasks previously deemed psychometrically inadequate.\n\n\n\n\n\n\n\nI make assumptions (too)!\n\n\n\nI assume you:\n\nare vaguely familiar with hierarchical linear models (HLM) and Bayesian statistics 1.\nknow basic concepts in psychometrics - mainly the idea of “reliability”.\n\nThis post uses R with a tidyverse approach. My goal is to show how others’ great theoretical works can be applied in brms. If you’re already a fan, welcome. If not, get ready to fall in love!"
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#why-are-we-here",
    "href": "posts/DotProbeReliability/index.html#why-are-we-here",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "",
    "text": "Goal: Discuss two methods to detect reliability (of cognitive tasks in this case) using Bayesian hierarchical linear models (HLMs) in brms.\nCase Study: The Dot-Probe Task. Contrary to a recent publication that claims the emotional dot-probe task is not a reliable measure, I’ll show that more nuanced modeling suggests the task is probably “fine” reliability-wise.\nThe Bottom Line: This tutorial-like post demonstrates how Bayesian HLM can fundamentally change reliability assessment. By analyzing trial-level data rather than aggregated scores, these methods can uncover meaningful signal where traditional approaches see only noise—potentially rehabilitating numerous tasks previously deemed psychometrically inadequate.\n\n\n\n\n\n\n\nI make assumptions (too)!\n\n\n\nI assume you:\n\nare vaguely familiar with hierarchical linear models (HLM) and Bayesian statistics 1.\nknow basic concepts in psychometrics - mainly the idea of “reliability”.\n\nThis post uses R with a tidyverse approach. My goal is to show how others’ great theoretical works can be applied in brms. If you’re already a fan, welcome. If not, get ready to fall in love!"
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#a-blessing-and-a-curse---the-reliability-paradox",
    "href": "posts/DotProbeReliability/index.html#a-blessing-and-a-curse---the-reliability-paradox",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "A Blessing and a Curse - the “Reliability Paradox”",
    "text": "A Blessing and a Curse - the “Reliability Paradox”\nScientists love finding significant results. Cognitive psychologists, in particular, aim to build “robust tasks”—ones that consistently produce similar significant effects when averaged across participants. When participants perform more similarly (showing less between-subject variance), effects tend to be more stable and therefore significant, leading to happy researchers. But here’s the ironic twist: to get that consistent effect, tasks often get designed or refined until nearly all participants show the same pattern. In other words, cognitive psychologists (perhaps inadvertently) end up with tasks that minimize individual differences.\nThis is a thorny situation that reveals itself when researchers try to correlate their robust tasks performance with other measures of individual differences like questionnaires, or performance on other tasks. That’s when they discover there simply isn’t enough variability between participants to work with and to differentiate between their subjects.\nThe complexity worsens when the sought-after effect is a difference between two conditions of the same task (i.e., a within-subject condition). Take, for instance, when we measure attentional bias by comparing how quickly people respond to a probe that appears behind threatening versus neutral images—in this case researchers subtract one reaction time (RT; the score from the threatening stimuli) from another (the score from the neutral stimuli) in what’s known as the dot-probe task. This introduces the “reliability of difference scores” problem into play—a well-documented challenge in measuring and understanding change. For both theoretical and statistical reasons, subtracting two within-subjects conditions produces individual difference scores with poor psychometric properties. These scores typically show weak correlations with themselves both when taken across time points (i.e., low test-retest reliability) and within a single administration (i.e., low internal reliability). And here is where the thorn hits again, because if these measures can’t correlate with themselves, how (in the hell) can they correlate with anything else?\nThis tension between “robust tasks” (good for group-level effects) and high reliability (good for individual differences) is the heart of the so-called “reliability paradox,” as popularized by Hedge et al. and Draheim et al.. These well-rounded and accessible papers explore this paradox in depth, making further elaboration here unnecessary. The key insight here is that, there is an inherent pitfall in the meeting point of the endeavor after robust tasks, difference scores and individual differences."
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#on-repelling-the-reliability-curse",
    "href": "posts/DotProbeReliability/index.html#on-repelling-the-reliability-curse",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "On repelling the reliability curse",
    "text": "On repelling the reliability curse\nThere are generally two approaches to addressing the reliability paradox:\n\nBetter task design – creating more challenging tasks or developing multiple ways to tap into the construct of interest, thereby generating more within-subjects variation. This direction has already yielded promising results (for example).\nNuanced Statistical modeling - analyzing trial-by-trial data with a model (often Bayesian HLM) that explicitly teases out and retains individual differences.\n\n\nOn Bayesian Hierarchical Models\nI won’t re-argue the entire case for Bayesian HLMs here (see footnote 1, these great materials, and don’t miss Nathaniel Haines blog). Suffice it to say that:\n\nTrial-Level modeling: Bayesian HLMs are generative models that we will feed trial-by-trial data. This means our models produce informed distributions for each participant in each condition. By considering trial-by-trial variation through this model-informed approach, we get nuanced measures of individual performance—precious information that would be lost in traditional ANOVA-style aggregation.\nDistributional Flexibility: Traditional linear models or ANOVAs assume our measures ultimately follow a Gaussian distribution. With Bayesian HLMs, we can choose distributions that actually match our data. Take RTs, for instance—they’re right-skewed and always positive. We can model this reality using a lognormal distribution (more on this later), allowing individual distributions to follow suit. This capability is particularly valuable for individual differences research.\nPosterior Distribution: the Bayesian philosophy itself offers a distinct advantage. Instead of getting a single reliability value, we get posterior distributions—entire spectrums of possible values. This provides a richer, more nuanced perspective on reliability in our data."
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#the-case-study-revisiting-the-dot-probe-task",
    "href": "posts/DotProbeReliability/index.html#the-case-study-revisiting-the-dot-probe-task",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "The Case Study: Revisiting the Dot-Probe Task",
    "text": "The Case Study: Revisiting the Dot-Probe Task\nXu et al. (2024) have recently provided a compelling case study. They set out to test whether the “emotional dot‐probe task”—a long-standing paradigm in cognitive psychology—can truly capture an attentional bias to threat with any consistency. In a typical trial of the dot-probe task, two images (one threatening, one neutral) are flashed on opposite sides of the screen. After a brief interval, these images disappear and a target (often a letter like “E” or “F”) appears in the location of one of the images. The idea is that if you’re quicker to respond when the target replaces the threat, you’ve got a bias toward threat. Xu et al. took this classic design further by testing 36 variations of the task—tweaking everything from stimulus type (faces, scenes, snakes/spiders) to stimulus orientation (horizontal vs. vertical) and the timing of the stimulus onset asynchronies (SOAs; 100, 500, or 900 ms). Their stark finding: almost all variations of the task produced internal reliability estimates that were essentially zero, suggesting that this task holds little promise for detecting individual differences.\nIn this post, I reanalyze a subset of Xu et al.’s data with Bayesian HLM, hypothesizing that a more sophisticated approach might rescue some of that lost individual variance.\n\n\n\n\n\n\nPre-processing the Data\n\n\n\n\n\nI took the large datasets from Xu et al. and made several key decisions:\n\nFocusing on Study 1: This study closely examined the reliability of the emotional dot‐probe task, making it ideal for reanalysis.\nSelecting Task Variants: To ensure a substantial sample size, I combined data from six variants of the task (out of the 36 tested in the paper). All used faces as threatening stimuli on vertical display, differing in their SOAs and whether additional neutral trials were included. Only threat-congruent (TC) and threat-incongruent (TIC) trials were analyzed.\nData Cleaning and Filtering: Building on Xu et al.’s exclusion criteria—participants with less than 60% accuracy or median RTs under 300 ms—I added a few more:\n\n\nIncluded only mouse responses to reduce device-related variability.\nRetained only correct response.\nExcluded responses faster than 250 ms (likely anticipatory) and slower than 3500 ms (potential distractions).\nOutliers control: I removed trials where RTs deviated more than 3.29 median absolute deviations from the median per subject and condition.\n\nThese are quite conservative steps. While many may not be strictly necessary for individual differences studies, they are standard in classic cognitive task analyses. Our total sample size now is 698 participants (!).\nHere is the R code for the preprocessing:\n\nlibrary(tidyverse)\n\n`%ni%` &lt;- Negate(`%in%`)\n\nStudy_1 &lt;- readr::read_csv(\"openData_study1_trials_102823.csv\")\ns1.all.outcomes &lt;- readr::read_csv(\"openData_study1_outcomes_102823.csv\")\n\n# Exclude participants with &lt;60% accuracy or median RT &lt;300 ms\ns1.excluded.id &lt;- s1.all.outcomes %&gt;%\n  filter(all_accuracy &lt; 0.6 | all_medRTc &lt; 300) %&gt;%\n  pull(id)\n\n\nStudy1_fs &lt;- Study_1 %&gt;%\n  filter(id %ni% s1.excluded.id, \n         condition != \"practice\", \n         resp_type != \"timeout\") %&gt;%\n  filter(test_id %in% c(1:3, 19:21), \n         resp_type == \"mouse\") %&gt;% \n  mutate(\n    RT = as.numeric(rt) / 1000,  # Convert RT to seconds\n    accuracy = as.numeric(correct),\n    trial.all.type = row_number(), \n    split = as.factor(ifelse(trial.all.type %% 2 == 1, 1, 2)), \n    condition = case_when(\n      condition == 1 ~ \"TC\",\n      condition == 2 ~ \"TIC\",\n      condition == 3 ~ \"TT\",\n      condition == 4 ~ \"NN\"\n    )\n   ) %&gt;%\n   filter(RT &gt; 0.25, RT &lt; 3.5, accuracy == 1, condition %in% c(\"TC\", \"TIC\")) %&gt;%\n   group_by(id, condition) %&gt;%\n   mutate(Outlier = c(datawizard::standardize(RT, robust = TRUE))) %&gt;%\n   ungroup() %&gt;% \n   filter(abs(Outlier) &lt; 3.29)"
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#rouder-haaf-model-in-brms",
    "href": "posts/DotProbeReliability/index.html#rouder-haaf-model-in-brms",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "Rouder-Haaf Model in brms",
    "text": "Rouder-Haaf Model in brms\nNow that we understand the conceptual framework, let’s see how to implement this in brms. The model specification needs to be careful and precise - we’re dealing with reaction time data that has specific characteristics, and we want to capture best the nature of our measurements:\nlibrary(brms)\nlibrary(bayestestR)\n\noptions(contrasts = c(\"contr.equalprior\", \"contr.poly\"))\n\npriors &lt;- c(\n  prior(exponential(1), class = \"sd\", group = \"id\"),\n  prior(exponential(1), class = \"sd\", group = \"id\", dpar = \"sigma\"),  \n  \n  \n  prior(normal(0, 1), class = \"b\", dpar = \"sigma\"),  \n  \n  # Fixed effects\n  prior(normal(0, 0.1), class = \"b\"))\nWe start by loading the necessary packages and setting proper sum to zero contrast coding. The contr.equalprior ensures unbiased Bayesian estimation when working with factors - unlike traditional effect or treatment coding which can lead to biased priors (see here for a detailed explanation).\nNext, we define relatively weakly informative priors for all parameters. The random-effects standard deviations (for both the mean and the residual variance) receive exponential(1), reflecting mild assumptions about individual variability. Meanwhile, the fixed effects for the mean RT are assigned a normal(0, 0.1) prior, implying modest expected effect sizes. Finally, we allow for some uncertainty in the residual standard deviation by placing a normal(0, 1) prior on any parameters governing sigma. These choices are conservative enough to encourage stable estimation, yet flexible enough to capture meaningful individual differences in response times.\nDotProbeModel.RouderHaafModel.RDS &lt;- brm(\n  formula = bf(\n    RT | trunc(lb = 0.25, ub = 3.5) ~ \n      condition * split +\n      (condition * split|p|id),\n    \n    sigma ~ \n      condition * split +\n      (condition * split|p|id)\n    ),\n  prior = priors,\n\n  family = lognormal(), data = Study1_fs,\n\n  iter = 4000, warmup = 2000,\n\n  chains = 4,  cores = 4,\n\n  threads = threading(2),\n\n  control = list(adapt_delta = 0.95,\n                 max_treedepth=12),\n\n  init = 0, backend = \"cmdstanr\")\nThe model formula contains several important features:\n\nTruncation: trunc(lb = 0.25, ub = 3.5) tells the model our response variable (RT) cannot fall below 0.25 seconds or exceed 3.5 seconds. This matches our preprocessing decisions and helps the model make more accurate predictions by respecting the actual bounds of our data.\nDistribution Choice: family = lognormal() is crucial. Reaction times follow a characteristic right-skewed distribution—they can’t be negative, tend to have a longer right tail, and the variability often increases with the mean. Importantly, the lognormal distribution captures these properties naturally by modeling RTs on the log scale, where effects are multiplicative (e.g., a 10% slowdown instead of ±X ms).\nThis multiplicative framework elegantly handles proportional relationships (e.g., a participant slowing by 10% in threat trials retains the same effect size whether their baseline is 500 ms or 1000 ms). In the same vein, reliability estimates derived from the log scale reflect proportional differences. Nevertheless, it is mostly common to report and use absolute differences in raw milliseconds. Later, we’ll reconcile this by converting our reliability estimates to the raw RT scale using posterior predictions—ensuring consistency with Xu et al.’s approach and broader psychometric conventions.\nThe Sigma: The sigma ~ condition * split + (condition * split | p | id) part of the formula recognizes that people aren’t just different in their overall consistency, but that their variability can also differ by condition and split. Imagine some participants who remain steady across all conditions, while others might fluctuate more in one condition than another. By letting each participant have their own “baseline consistency” as well as condition- and split-specific effects, we are not just acknowledging that people vary overall—we’re allowing these differences to manifest across different experimental factors, rather than forcing everyone into a single error structure.\nThe lognormal distribution already accounts for the fact that reaction times naturally “spread out” as they lengthen (e.g., a 3-second average often has bigger ups/downs than a 1-second average). Here, we go a step further: each person can have a distinct sigma profile for each combination of condition and split—capturing a richer picture of individual differences.\nCorrelated Individual Effects: The |p| syntax in both the main formula (condition * split |p|id) and the sigma formula(condition * split |p|id) allows the model to account for an important reality of reaction time data: participants with longer RTs typically also show larger variability in their responses. Instead of assuming independent effects, this structure lets the model estimate correlations between individual differences in mean RTs and response variability. By doing so, it refines individual estimates even further.\n\n\nExtracting the reliability\nJust before examining the reliability using a full Bayesian approach à la “Rouder and Haaf”, let’s look at what we get using the classic method for calculating internal consistency.\n\n\n\n\n\n\n\n\n\nThe results mirror those reported by Xu et al., showing a nearly zero correlation of -0.019 (and similarly poor results after applying the Spearman-Brown correction) – in fact, slightly negative in this case.\nNow for the main course: Rouder and Haaf reliability estimate. Using tidybayes, I extract the standard deviations of the random effects for both the congruity effect and the interaction and transforming them to the response scale.\nVar.df &lt;- DotProbeModel1 %&gt;%\n  spread_rvars(sd_id__trial_type1, `sd_id__trial_type1:half.all.type1`)\n  \nVar.df &lt;- Var.df %&gt;% \n  mutate(\n  sd_response_condition1_response = exp(sd_id__condition1^2 / 2) * sqrt(exp(sd_id__condition1^2) - 1),\n  sd_response_interaction_response = exp(`sd_id__condition1:split1`^2 / 2) * sqrt(exp(`sd_id__condition1:split1`^2) - 1)\n  )\nI square the standard deviations in order to get variance,\nVar.df_variance &lt;- Var.df %&gt;%\n  mutate(across(everything(), ~ .^2)) \nand then what I have left is to perform the subtraction and division (the rvars structure makes this remarkably straightforward):\nRouder.Haaf.numerator &lt;- \n  Var.df_variance$sd_id__trial_type1 - \n  Var.df_variance$`sd_id__trial_type1:half.all.type1`\n\nRouder.Haaf.denominator &lt;- \n  Var.df_variance$sd_id__trial_type1 + \n  Var.df_variance$`sd_id__trial_type1:half.all.type1`\n\nRouder.Haaf.Reliability &lt;- Rouder.Haaf.numerator/Rouder.Haaf.denominator\n\n\n\n\n\n\n\n\n\nThis is the posterior reliability coefficient distribution—a distribution of possible values for the reliability coefficient we are estimating, where the density reflects how likely each value is.\nWhat emerges here, is in my opinion, a fascinating picture of informative Bayesian complexity:\n\nFirst, we see the remarkable benefits of the Bayesian HLM approach, where 99.96% of the possible reliability values exceed the raw-aggregated value of -0.019. This dramatic difference highlights how traditional aggregation methods might severely underestimate the task’s reliability.\nAt the same time, the actual reliability of the Dot-probe task isn’t definitively established. The 95% credible interval suggests reliability values range from 0.43 to a practically perfect 1. While this wide posterior distribution indicates considerable uncertainty about the task’s reliability, it also allows us to make informed probability statements about specific thresholds. Based on the posterior distribution, there’s a 85.72% chance that the Dot-probe task reliability exceeds 0.6, a 77.65% chance it exceeds 0.7, and a 65.79% chance it exceeds 0.8. Such nuanced probabilistic conclusions aren’t possible with classic models and give researchers the tools to make reasoned decisions.\nBut what’s the most likely reliability? The Maximum A Posteriori probability estimate (MAP) point estimates follows for the most likely value and in this case indicates on practically perfect reliability of 0.998. More conservative estimates include the median at 0.892 and the mean at 0.823. All these point estimates suggest the task is substantially more reliable than previously thought, with even the most conservative estimate showing strong reliability.\n\nReminder: our example deals with “split-half” reliability, a type of “internal consistency”. It typically exceeds what you’d get with, for example, a test–retest scenario (different sessions, potential fatigue effects, etc.), so you might consider the values here as an upper bound on broader “reliability”."
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#implementing-chen-et-al.-in-brms",
    "href": "posts/DotProbeReliability/index.html#implementing-chen-et-al.-in-brms",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "Implementing Chen et al. in brms",
    "text": "Implementing Chen et al. in brms\nChen et al. take a distinct modeling approach by focusing on how condition effects (TC vs. TIC) vary across splits (odd vs. even trials). Their model omits both the global intercept and main effect of condition, instead modeling how these condition differences manifest through interactions with splits. This parameterization directly captures how stable individual differences in condition effects are across different portions of the task. Here’s how we implement this approach in brms. Rather than using a standard factorial structure (condition + split + condition:split), we separate the random effects into distinct components for splits and condition-by-split interactions:\nDotProbeModel.ChenModel &lt;- brm(\n  formula = bf(\n    RT | trunc(lb = 0.25, ub = 3.5) ~ 0 +\n      split+\n      condition:split+\n      (0+split|p|id)+\n      (0+split:condition|p|id),\n    \n    sigma ~ 0 +\n      split+\n      condition:split+\n      (0+split|p|id)+\n      (0+split:condition|p|id)\n  ),\n  prior = priors,\n\n  family = lognormal(), data = Study1_fs,\n\n  iter = 4000, warmup = 2000,\n\n  chains = 4,  cores = 4,\n\n  threads = threading(2),\n\n  control = list(adapt_delta = 0.95,\n                 max_treedepth=12),\n\n  init = 0, backend = \"cmdstanr\")\n\nExtracting the Reliability\nThe Chen et al. approach gives us a reliability estimate by correlating threat bias scores (TIC-TC) between splits. The process mirrors what we typically do in reliability analysis: we compute difference scores for each split level (odd vs even trials) and correlate them, but with an important Bayesian twist - we do this for every posterior draw from our model, giving us a full distribution of reliability estimates instead of just one number. I built on their approach by adding the Spearman-Brown correction - a standard adjustment in psychometrics that accounts for split-half calculations. We need this correction because we’re essentially working with half the data in each split, and it makes our estimates more comparable to full-test reliability. Plus, since Xu et al. used this correction in their original analysis, it lets us make direct comparisons.\n\n# Simulate expected RTs (milliseconds) for all splits/conditions\nnewdata &lt;- tidyr::expand_grid(\n  id = unique(Study1_fs$id),        \n  split = c(\"1\", \"2\"),         \n  condition = c(\"TIC\", \"TC\")          \n)\n\npred &lt;- add_epred_rvars(\n  DotProbeModel.ChenModel,\n  newdata = newdata,\n  re_formula = NULL           \n) %&gt;% \n  mutate(condition = interaction(split, condition, sep = \"_\")) \n\n# Compute threat bias (TIC - TC) for each split\ndiff_scores &lt;- pred %&gt;%\n  select(id, condition, .epred) %&gt;%\n  pivot_wider(names_from = condition, values_from = .epred) %&gt;%\n  mutate(\n    Effect_1 = `1_TIC` - `1_TC`,  # Split 1 effect (ms)\n    Effect_2 = `2_TIC` - `2_TC`   # Split 2 effect (ms)\n  ) \n\n# Calculate reliability per posterior draw\ncor_results &lt;- \n  data.frame(\n    correlation =  \n      with(diff_scores, {\n        rdo(\n          cor(Effect_1, Effect_2)\n          )})) %&gt;%\n  mutate(\n    spearman_brown = (2 * correlation) / (1 + correlation)\n    )\n\n\n\n\n\n\n\n\n\nThe posterior distributions reveal 95% credible intervals ranging from 0.41 to 0.81 for the raw correlation, and from 0.59 to 0.91 for the Spearman-Brown corrected reliability, with median estimates of 0.62 and 0.77, respectively. These values suggest moderate to strong reliability, though somewhat lower than the Rouder-Haaf estimates. This difference is instructive – while both approaches support the task’s reliability, they do so through different lenses. The Chen et al. method provides more conservative estimates that may feel more familiar to researchers used to traditional reliability metrics, while still demonstrating substantially higher reliability than previously reported using simpler methods."
  },
  {
    "objectID": "posts/DotProbeReliability/index.html#footnotes",
    "href": "posts/DotProbeReliability/index.html#footnotes",
    "title": "The Dot-Probe Task is Probably Fine",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’re wondering how to familiarize yourself with this material, I recommend starting with the excellent book “A Student’s Guide to Bayesian Statistics” by Ben Lambert, which offers a fantastic introduction, especially for those with some understanding of frequentist statistics. Now, I’m a spiral learner—I often need to revisit concepts from different perspectives. With that in mind, I suggest following up with “Statistical Rethinking” and/or “Doing Bayesian Data Analysis”, alongside the legendary bookdowns by Solomon Kurz — As the title of the callout probably reveals, I am a fan of his writings.↩︎"
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html",
    "href": "posts/Logistict.ROPE/Beyond!!!.html",
    "title": "Beyond the Exclamation Points!!!",
    "section": "",
    "text": "Goal: Demonstrate a clearer approach to interpreting logistic regression using Bayesian methods and HDI-ROPE analysis, illustrated through real-world SMS spam detection.\nCase Study: Predicting whether an SMS message is spam based on linguistic toxicity patterns (captured through NLP+PCA), message length, and punctuation usage.\nThe Bottom Line: This tutorial shows how Bayesian modeling combined with marginal effects and HDI-ROPE analysis creates a more intuitive workflow for binary outcome analysis—avoiding the notorious “log-odds” interpretation problem while tackling the practical challenge of spam detection.\n\n\n\n\n\n\nI make assumptions (too)!\n\n\n\nI assume you:\n- Have basic familiarity with R and the tidyverse.\n- Understand the fundamentals of regression analysis.\n- Have encountered logistic regression before and familiar with the interpretation frustration."
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#why-are-we-here",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#why-are-we-here",
    "title": "Beyond the Exclamation Points!!!",
    "section": "",
    "text": "Goal: Demonstrate a clearer approach to interpreting logistic regression using Bayesian methods and HDI-ROPE analysis, illustrated through real-world SMS spam detection.\nCase Study: Predicting whether an SMS message is spam based on linguistic toxicity patterns (captured through NLP+PCA), message length, and punctuation usage.\nThe Bottom Line: This tutorial shows how Bayesian modeling combined with marginal effects and HDI-ROPE analysis creates a more intuitive workflow for binary outcome analysis—avoiding the notorious “log-odds” interpretation problem while tackling the practical challenge of spam detection.\n\n\n\n\n\n\nI make assumptions (too)!\n\n\n\nI assume you:\n- Have basic familiarity with R and the tidyverse.\n- Understand the fundamentals of regression analysis.\n- Have encountered logistic regression before and familiar with the interpretation frustration."
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#the-messages-behind-the-data-understanding-sms-spam-in-context",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#the-messages-behind-the-data-understanding-sms-spam-in-context",
    "title": "Beyond the Exclamation Points!!!",
    "section": "The Messages Behind the Data: Understanding SMS Spam in Context",
    "text": "The Messages Behind the Data: Understanding SMS Spam in Context\nImagine receiving a text message: “URGENT!!! You have WON £1,000,000!!! Reply NOW with your bank details!!!”\nYour brain instantly recognizes this as spam—but how? It’s not just excessive exclamation points or too-good-to-be-true offers. There’s a complex pattern of linguistic signals distinguishing legitimate messages from spam, which can vary significantly across cultural and linguistic contexts.\nThe dataset we’re exploring comes from the ExAIS SMS Spam project conducted in Nigeria, featuring 5,240 SMS messages (2,350 spam, 2,890 ham) collected from university community members aged 20–50.\nThe data contain the SPAM/HAM (not spam) classification and the text message itself. Using NLP magic, dimensionality reduction, and text analysis, I extracted some additional features:\n\nis_spam - Our outcome variable; a binary classification indicating spam or legitimate communication.\npc_aggression & pc_incoherence - Principal components I extracted from Google’s Perspective API toxicity scores capturing sophisticated linguistic patterns:\n\npc_aggression: threatening, toxic, and insulting language patterns.\npc_incoherence: inflammatory, incoherent, or unsubstantial messages.\n\nword_count - The total number of words per message (includes squared term to capture non-linear relationships).\nexclamation_count - The number of exclamation points, a simple but telling spam feature.\n\nOur central question is this: How do linguistic toxicity patterns (captured by PCA components) interact with message characteristics like length and punctuation to identify spam? And more importantly, how can we interpret these relationships meaningfully?\nIn the note below you can find the full pipeline with code for the NLP and PCA code.\n\n\n\n\n\n\nA Note on Advanced NLP Features\n\n\n\n\n\nBelow is the exact, reproducible pipeline I used to transform raw text into the two tidy principal‑component dials (pc_aggression, pc_incoherence) you saw in the main post. Everything is wrapped in code‑chunks so you (or your future self) can copy‑paste the whole block into Quarto/R Markdown and run it end‑to‑end.\n\n# ── 1 · Load required packages ───────────────────────────────────────────\nlibrary(tidyverse)     # data manipulation and pipes\nlibrary(peRspective)   # R client for Google/Jigsaw Perspective API\nlibrary(FactoMineR)    # Principal‑Component Analysis\nlibrary(pROC)          # ROC curves (for later evaluation)\n\n\nWhat is peRspective?\npeRspective is a thin, tidyverse‑friendly wrapper around the Perspective API. It takes care of batching requests, retrying on rate‑limits, and returning scores as a clean data frame. Before running the chunk below you’ll need a free API key (set it once with Sys.setenv(PERSPECTIVE_API_KEY = \"&lt;your‑key&gt;\")).\n\n\nStep 1 – Obtain linguistic scores from the Perspective API\nEach message is sent to the API, which returns up to nine probabilities indicating the presence of attributes such as toxicity, threat, or incoherence.\n\n# ⚠️  Disabled by default to avoid accidental quota use.\nperspective_scores &lt;- dataset_clean %&gt;%\n  prsp_stream(\n    text        = message,          # column containing the SMS text\n    text_id     = text_id,          # unique identifier for safe joins\n    score_model = c(\"THREAT\", \"TOXICITY\", \"INSULT\", \"SPAM\",\n                    \"INFLAMMATORY\", \"INCOHERENT\", \"UNSUBSTANTIAL\",\n                    \"FLIRTATION\", \"PROFANITY\"),\n    safe_output = TRUE,             # masks content the API flags as unsafe\n    verbose     = TRUE)             # progress messages\n\n\nDevelopment tip:\nWhen iterating on downstream scripts, save the scores once (write_csv()) and reload them to avoid repeated network calls:\n\nperspective_scores &lt;- read_csv(\"~/perspective_scores_saved.csv\")\n\n\n\n\nStep 2 – Merge, clean, and prepare for PCA\nOccasionally a request fails or returns NA. We drop the few problematic rows and replace any missing attribute scores with zeros so PCA receives a complete numeric matrix.\n\nscored_data &lt;- dataset_clean %&gt;%\n  left_join(perspective_scores, by = \"text_id\") %&gt;%\n  filter(!(has_error = (!is.na(error) & error != \"No Error\"))) %&gt;%\n  mutate(across(THREAT:PROFANITY, ~replace_na(.x, 0))) %&gt;%\n  select(-SPAM, -FLIRTATION)   # remove two attributes found redundant here\n\n\n\nStep 3 – Reduce nine correlated scores to two principal components\nPrincipal‑Component Analysis (PCA) rotates the nine‑dimensional attribute space until the first component captures the largest systematic variation, the second captures the next‑largest, and so on.\n\npca_result &lt;- PCA(scored_data %&gt;% select(THREAT:PROFANITY),\n                  ncp   = 3,    # keep three components for inspection\n                  graph = FALSE)\n\npca_scores &lt;- as_tibble(pca_result$ind$coord) %&gt;%\n  set_names(c(\"pc_aggression\",     # roughly: threat + toxicity + insult\n              \"pc_inflammatory\",   # moderate mix (not used later)\n              \"pc_incoherence\")) %&gt;%   # roughly: incoherent + unsubstantial\n  mutate(text_id = scored_data$text_id)\n\n\npc_aggression ranges from neutral tone to overt hostility.\npc_incoherence tracks the continuum from cohesive message to nonsensical or fragmentary text.\n\nThese two components retain ≈ 72 % of the total variance in the original nine attributes, providing a compact yet informative description of each message.\n\n\nStep 4 – Assemble the modelling table\nFinally, we merge the PCA scores back with the pre‑computed surface features (word_count, exclamation_count, etc.) so the eventual model can consider both linguistic tone and formatting cues.\n\nmodel_data &lt;- scored_data %&gt;%\n  select(-THREAT:-PROFANITY) %&gt;%\n  left_join(pca_scores, by = \"text_id\")\n\nAt this point model_data is a tidy, analysis‑ready data frame: one row per SMS, interpretable columns for tone and structure, and no missing values to trip up downstream methods."
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#the-interpretation-challenge-why-binary-outcomes-are-tricky",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#the-interpretation-challenge-why-binary-outcomes-are-tricky",
    "title": "Beyond the Exclamation Points!!!",
    "section": "The Interpretation Challenge:  Why Binary Outcomes Are Tricky",
    "text": "The Interpretation Challenge:  Why Binary Outcomes Are Tricky\nLogistic regression dominates binary outcome modeling, but before diving into its interpretation challenges, let’s see why we can’t just use the familiar linear regression like:\n\\[\n\\text{spam} \\;=\\; \\beta_0 \\;+\\; \\beta_1 \\times \\text{aggression}\n                 \\;+\\; \\beta_2 \\times \\text{word count} \\;+\\; \\dots\n\\]\nLet me demonstrate with some simulated spam data.\nThe first problem is that linear regression fundamentally misunderstands the nature of binary data. Let’s examine the posterior predictive checks—these diagnostic plots ask “if our model were true, what would data look like?” By generating many fake datasets from each model and comparing them to reality, we can see whether the model truly grasps the data-generating process:\n\n\n\n\n\nLook at the stark mismatch! Our actual data (gray lines) consists solely of 0s and 1s—spam or not spam, no middle ground. Here’s the crucial distinction: while both models predict probabilities internally (e.g., “this message has 70% chance of being spam”), this plot shows what type of observed data each model’s mathematical structure implies. The logistic model (green bars) is specified for binary outcomes—it uses its internal probabilities to generate realistic 0s and 1s, creating those two distinct spikes. The linear model (red distribution), however, assumes continuous outcomes and generates spam “scores” spread out in a bell curve, as if we could observe a message being “0.7 spam.”\nThis mismatch isn’t trivial—it fundamentally undermines the model’s validity for binary outcomes. When a model’s mathematical structure doesn’t match the basic nature of your outcome variable, its estimates of relationships become suspect. The linear model’s equations assume continuous variation that doesn’t actually exist in binary data. The effect sizes, confidence intervals, and predictions all come from a model that by definition grasps the data wrongfully.\nBut the problems run deeper. Let’s look at specific predictions across the range of our predictors:\n\n\n\n\n\n\n\n\n\nThe linear model draws a straight line that blissfully crosses into impossible territory, predicting probabilities well above 100% for messages with high aggression scores. This isn’t a minor edge case—any linear model with meaningful predictors will eventually produce impossible predictions at the extremes. It’s mathematically inevitable when you try to force a straight line onto a bounded outcome.\nAmong the many ways linear regression fails for binary data, we’ve seen two critical issues: it misunderstands the nature of the outcome (expecting continuous values when only 0s and 1s exist) and produces nonsensical predictions that escape probability bounds.\n\nHow Logistic Regression Fixes It\nThe mathematical solution of the (in)famous “S-curve” formula goes as follow:\n\\[p(\\text{spam}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\times \\text{aggression} + \\beta_2 \\times \\text{word count} + ...)}}\\]\nInside the parentheses is still a straight-line blend of your predictors; the logistic function simply transforms this into a valid probability between 0 and 1.\n\n\n\n\n\n\n\n\n\nThe S-shaped curve elegantly solves both our problems: it respects the binary nature of data and keeps predictions within valid probability bounds. But this solution creates a new challenge: the curve’s varying steepness means that the effect of any predictor depends on where you are on the curve. A one-unit increase in aggression might change spam probability by 20 percentage points in the middle of the curve but only 2 percentage points near the edges.\n\n\nA Peek Behind the Curtain: Log-Odds\nTo understand why interpretation becomes tricky, we need to peek at how logistic regression actually works. It achieves that S-curve by working in a transformed space called “log-odds.”\nThink of it as changing measuring units—like converting temperature from Celsius to Fahrenheit, but for probabilities:\n\nOdds re-phrase probability: 30% chance of spam is equal in odds = 0.3 / 0.7 ≈ 0.43.\nLog-odds take those odds and apply a logarithm. This stretches the probability scale: 0% becomes negative infinity, 100% becomes positive infinity, and everything else spreads out smoothly in between.\n\nOn this stretched-out log-odds scale, we can finally draw our straight line (the right panel):\n\\[\n\\log\\!\\Bigl(\\tfrac{p}{1-p}\\Bigr)\n  \\;=\\;\n  \\beta_0 \\;+\\; \\beta_1 \\times \\text{aggression}\n           \\;+\\; \\beta_2 \\times \\text{word count}\n           \\;+\\; \\dots\n\\]\nThe left panel shows the jittered 0/1 spam labels (grey points) alongside the model’s predicted probability\n\\(\\hat{p} = p(\\text{spam}=1)\\), plotted as a solid blue curve on \\([0,1]\\).\nThe right panel displays the same model on the log-odds scale with a dashed red line for\n\\(\\operatorname{logit}(\\hat{p}) = \\ln\\!\\bigl(\\tfrac{\\hat{p}}{1-\\hat{p}}\\bigr)\\), which straightens the S-shaped curve.\nIn both panels, grey arrows trace one example predictor value and illustrate how it maps from the probability curve to the straight line in log-odds space."
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#why-bayesian",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#why-bayesian",
    "title": "Beyond the Exclamation Points!!!",
    "section": "Why Bayesian?",
    "text": "Why Bayesian?\nThe marginaleffects package beautifully transforms model results into interpretable probability statements. More on that later. I will not stop here but also combine it with Bayesian estimation to create an even more powerful analytical framework.\nBayesian methods solve several technical problems that plague binary outcome models:\n\nComplete Seperation Issues\nThe Problem: Complete separation occurs when a predictor perfectly divides outcome categories. Imagine if every message with more than 10 exclamation points was spam while no message with fewer was—traditional maximum likelihood estimation would produce infinite coefficient estimates.\nThe Bayesian Solution: Priors act as natural regularizers, keeping estimates finite and meaningful even in extreme cases. Instead of model failure, we get sensible uncertainty quantification around our estimates. This is particularly important in spam detection where new tactics constantly emerge, potentially creating separation in specific feature combinations.\n\n\nRobust Computation\nThe Problem: Our model includes interactions between PCA components and text features—these complex relationships often cause convergence failures in traditional frameworks. Researchers are forced to simplify their models, potentially missing important patterns in how spam characteristics combine.\nThe Bayesian Solution: Modern implementations like brms handle complex model structures that would break optimization-based methods, letting us build models that match our theoretical questions rather than computational constraints.\nThis Bayesian foundation, combined with marginaleffects for interpretation, gives us the best of both worlds: robust estimation and intuitive communication of results."
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#making-bayesian-binary-models-practical-priors-and-inference",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#making-bayesian-binary-models-practical-priors-and-inference",
    "title": "Beyond the Exclamation Points!!!",
    "section": "Making Bayesian Binary Models Practical: Priors and Inference",
    "text": "Making Bayesian Binary Models Practical: Priors and Inference\nThere are two main obstacles that often discourage researchers from adopting Bayesian methods: choosing appropriate priors and interpreting inference from posterior distributions. My goal here is to show that both are surprisingly straightforward for logistic hierarchical models—especially when we combine the right tools.\n\nThe Prior Specification Problem\nWhen you fit a Bayesian logistic model in brms, your regression coefficients live on the log-odds scale. This creates an immediate headache: what does a “reasonable” prior look like for log-odds? Is normal(0, 1) too wide? Too narrow? It’s hard to have intuitions about log-odds because we don’t naturally think that way.\nBut here’s where things get clever. There’s a beautiful relationship between odds ratios and Cohen’s d (the standardized effect size we all learned about in intro stats). Sánchez-Meca, Marín-Martínez, and Chacón-Moscoso (2003) showed us this handy conversion:\n\\[d = \\log(OR) \\times \\frac{\\sqrt{3}}{\\pi}\\]\nRearranging this gives us:\n\\[\\log(OR) = d \\times \\frac{\\pi}{\\sqrt{3}}\\]\nThis is our golden ticket! Now we can think about our priors in terms of standardized effect sizes (small, medium, large) and convert them to the log-odds scale.\n\nA Practical Workflow for Prior Specification\nHere’s my step-by-step approach to setting sensible priors:\nStep 1: Think in Cohen’s d terms\nWe all know these benchmarks:\n\nSmall effect: d ≈ 0.2\nMedium effect: d ≈ 0.5\n\nLarge effect: d ≈ 0.8\n\nFor spam detection, most individual features probably have small to medium effects. It’s unlikely that any single characteristic (like exclamation count) has a massive effect on spam probability.\nStep 2: Convert to log-odds standard deviation\nIf we want our prior to be centered at zero (no effect) with most mass on small-to-medium effects, we can set the standard deviation of our normal prior using:\n\\[\\sigma_{\\log(OR)} = d \\times \\frac{\\pi}{\\sqrt{3}}\\]\nStep 3: Always scale your predictors\nThis is crucial! The interpretation of “one unit change” only makes sense if your predictors are on a comparable scale. I always use scale() to standardize continuous predictors.\nLet me show you how this works in practice:\n\n\n\n\n\n\n\n\n\nFor priors, I’ll introduce a clever approach in the next section that makes setting informed priors for logistic regression intuitive. But first, let’s tackle the inference challenge with an approach that makes traditional p-values look like apprentice-level magic.\n\n\n\nHDI+ROPE: A Bayesian Path to Statistical Inference\nTraditional null hypothesis significance testing (NHST) with p-values pushed us into a binary world: an effect is either “significant” or “not significant” based on an arbitrary threshold (typically p &lt; .05). This approach has been criticized extensively—not least because it collapses a continuous measure of evidence into a dichotomous decision. It’s like trying to describe a complex magical spell with just “works” or “fails”—when in reality, there’s a rich spectrum of possible outcomes.\nBayesian inference offers a more nuanced perspective through posterior distributions. Instead of a single p-value, we get an entire distribution of plausible parameter values. But this richness creates a new challenge: how do we make practical decisions without being overwhelmed by distributional complexity?\nEnter the HDI+ROPE decision rule—a powerful framework developed and championed by John Kruschke (2014, 2018) that provides a principled alternative to traditional significance testing while preserving the nuance of Bayesian inference.\n\nThe Highest Density Interval (HDI)\nThe HDI contains a specified percentage of the most probable parameter values, where every value inside the interval has higher probability density than any value outside. Unlike frequentist confidence intervals, the HDI has a direct probabilistic interpretation: given our data and model, there’s an X% probability that the true parameter value lies within the X% HDI (e.g., 95% probability for a 95% HDI).\nWhile Kruschke originally recommended using 95% HDIs (and later suggested 89% as potentially better), we’ll take advantage of the full posterior distribution (HDI 100%)—using the complete picture of uncertainty in our spell analysis rather than just the HDI boundaries.\n\n\nThe Region of Practical Equivalence (ROPE)\nThe ROPE is essentially us asking, “What effect sizes are so small that I wouldn’t care about them in practice?” For illustration, we might set our ROPE at ±5%—meaning any effect that changes concentration probability by less than 5 percentage points either way is considered practically negligible, though different analyses may warrant different ROPE boundaries.\nImportantly, ROPE represents a shift from traditional significance testing to effect size reasoning. Rather than asking “Is there any effect, no matter how small?” (significance testing), we ask “Is the effect large enough to matter?” (effect size evaluation). If a spell level increases concentration probability by 0.01%, that might be statistically detectable with enough data, but no game designer or player would ever notice or care. The ROPE lets us define this “too small to matter” range—distinguishing between effects that are statistically detectable versus practically meaningful for decision-making. This focus on effect magnitude rather than mere detectability represents a fundamental shift toward more substantive scientific inference.\n\n\nMaking Decisions with HDI+ROPE\nWhen using the full posterior distribution approach (recommended based on simulation studies), the decision rules are straightforward. Using our 5% example threshold (though other values may be appropriate for different contexts):\n\nReject the null hypothesis if less than 5% of the posterior distribution falls within the ROPE. This means we have strong evidence for a practically meaningful effect. The visualization below shows this as a distribution clearly extending beyond our equivalence boundaries.\nAccept the null hypothesis if more than 95% of the posterior distribution falls within the ROPE. This means we have evidence for the practical absence of an effect—the parameter is essentially equivalent to zero in gameplay terms. This appears as a distribution tightly concentrated within the equivalence zone.\nRemain undecided if the percentage falls between these thresholds. The evidence is inconclusive at our current precision level, shown as distributions that substantially span the ROPE boundaries—we need to gather more data from additional playtesting.\n\nUnlike p-values, which can only reject the null hypothesis but never support it, HDI+ROPE allows us to provide evidence for the absence of meaningful effects. This three-valued logic (effect exists, effect absent, undecided) better captures the reality of scientific evidence—sometimes we simply need more data to reach a conclusion.\nThe following visualization demonstrates these decision rules in action, showing four distinct patterns you might encounter when analyzing spell effects. Each scenario represents a different relationship between the posterior distribution and our example ±5% ROPE, illustrating how the same statistical framework can yield different conclusions depending on where the evidence falls. Notice how some effects might be statistically detectable (consistent small impacts) yet still fall within our practical equivalence zone—a nuance that traditional p-value approaches often miss.\nFor our D&D spell analysis, we’ll use this framework to draw substantive conclusions about what factors influence concentration requirements, with the understanding that evidence clustering within our ROPE suggests practically negligible effects, while distributions extending well beyond these boundaries indicate meaningful gameplay impacts."
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#the-interpretation-problem",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#the-interpretation-problem",
    "title": "Beyond the Exclamation Points!!!",
    "section": "The Interpretation Problem",
    "text": "The Interpretation Problem\nImagine explaining the model to a spam-filter engineer or product manager:\n\n“A one-unit increase in the aggression principal component raises the log-odds of spam by 0.85.”\n\nCue blank stares.\nSo we translate to odds ratios:\n\n“Each unit increase in aggression multiplies the odds of spam by 2.33 (e0.85).”\n\nStill murky! Even seasoned analysts struggle with odds because humans naturally think in probabilities, not odds. Add interaction terms (e.g., how aggression’s effect changes with message length) and interpretation gets even thornier.\nI’m going to put in tremendous effort to cut through that fog—re-expressing results exclusively on the familiar 0 %–100 % probability scale, and showing you practical tricks to keep interpretations clear in Bayesian statistics.\n\n\n\n\n\n\nBut Wait! Why Bayesian?\n\n\n\n\n\nThe marginaleffects package beautifully transforms model results into interpretable probability statements. More on that later. I will not stop here but also combine it with Bayesian estimation to create an even more powerful analytical framework.\nBayesian methods solve several technical problems that plague binary outcome models:\n\nComplete Separation Issues\nThe Problem: Complete separation occurs when a predictor perfectly divides outcome categories. Imagine if every message with more than 10 exclamation points was spam while no message with fewer was—traditional maximum likelihood estimation would produce infinite coefficient estimates.\nThe Bayesian Solution: Priors act as natural regularizers, keeping estimates finite and meaningful even in extreme cases. Instead of model failure, we get sensible uncertainty quantification around our estimates. This is particularly important in spam detection where new tactics constantly emerge, potentially creating separation in specific feature combinations.\n\n\nRobust Computation\nThe Problem: Our model includes interactions between PCA components and text features—these complex relationships often cause convergence failures in traditional frameworks. Researchers are forced to simplify their models, potentially missing important patterns in how spam characteristics combine.\nThe Bayesian Solution: Modern implementations like brms handle complex model structures that would break optimization-based methods, letting us build models that match our theoretical questions rather than computational constraints.\nThis Bayesian foundation, combined with marginaleffects for interpretation, gives us the best of both worlds: robust estimation and intuitive communication of results."
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#the-interpretation-challenge-n-why-binary-outcomes-are-tricky",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#the-interpretation-challenge-n-why-binary-outcomes-are-tricky",
    "title": "Beyond the Exclamation Points!!!",
    "section": "The Interpretation Challenge: /n Why Binary Outcomes Are Tricky",
    "text": "The Interpretation Challenge: /n Why Binary Outcomes Are Tricky\nLogistic regression is the go-to tool when the thing we’re trying to predict has only two possibilities—spam vs. not-spam, click vs. no-click, win vs. loss.\n\nWhy a Simple Straight Line Fails\nSuppose we try ordinary linear regression:\n\\[\np(\\text{spam}) \\;=\\; \\beta_0 \\;+\\; \\beta_1 \\times \\text{aggression}\n                 \\;+\\; \\beta_2 \\times \\text{word count} \\;+\\; \\dots\n\\]\nTwo big things go wrong:\n\nImpossible numbers\nA straight line can spit out 1.2 or –0.3, but probabilities can never exceed 1 or dip below 0.\nThat’s like predicting someone is “130 % pregnant.”\n\n\n\n\n\n\n\n\n\n\n\nUneven “noise”\nLinear regression works best when its errors—the gaps between truth and prediction—are\n\nbell-shaped and (b) roughly the same size everywhere.\n\nWith a 0 / 1 outcome that’s impossible:\n\nIf the true label is 0 and we predict (p), the error is (-p).\n\nIf the true label is 1, the error is (1-p).\n\nSo when (p ) the error can be as large as ±0.50, but when\n(p ) it shrinks to ±0.05.\nThe spread of the errors shrinks and stretches with (p)—a textbook case of heteroskedasticity.\nBecause ordinary regression assumes a constant error spread, its confidence intervals and (p)-values become shaky for binary data.\n\n\n\n\n\n\n\n\n\n\n\n\nHow Logistic Regression Fixes It\nLogistic regression keeps the linear part inside a curve that automatically squashes every prediction into [0, 1]:\n\\[p(\\text{spam}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\times \\text{aggression} + \\beta_2 \\times \\text{word count} + ...)}}\\]\nInside the parentheses is still a plain-vanilla linear combination; the logistic curve just keeps the final number honest.\n\n\nA Peek Behind the Curtain: Log-Odds\nTo make that magic work, the model actually does its sums on a hidden ruler called log-odds.\n\nOdds re-phrase probability:\n30 % chance of spam ⟷ odds = 0.3 / 0.7 ≈ 0.43 (about 3 : 7).\nLog-odds take those odds and apply a logarithm.\nProbability 0 maps to “minus infinity,” probability 1 to “plus infinity,”\nand everything in between spreads out on a tidy, unlimited scale.\n\nOn that log-odds ruler we can finally draw a straight line:\n\\[\n\\log\\!\\Bigl(\\tfrac{p}{1-p}\\Bigr)\n  \\;=\\;\n  \\beta_0 \\;+\\; \\beta_1 \\times \\text{aggression}\n           \\;+\\; \\beta_2 \\times \\text{word count}\n           \\;+\\; \\dots\n\\]\nAfter fitting that line, we bend it back into ordinary probabilities with the logistic curve, guaranteeing every prediction stays inside 0 %–100 %."
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#making-bayesian-binary-models-practical-from-priors-to-inference",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#making-bayesian-binary-models-practical-from-priors-to-inference",
    "title": "Beyond the Exclamation Points!!!",
    "section": "Making Bayesian Binary Models Practical: From Priors to Inference",
    "text": "Making Bayesian Binary Models Practical: From Priors to Inference\nThere are two main obstacles that often discourage researchers from adopting Bayesian methods: choosing appropriate priors and interpreting inference from posterior distributions. My goal here is to show that both are surprisingly straightforward for logistic hierarchical models—especially when we combine the right tools.\nLet’s build this model step by step, starting with prior specification:\n\nThe Prior Specification Problem\nWhen you fit a Bayesian logistic model in brms, your regression coefficients live on the log-odds scale (each β is a log-odds-ratio for a one-unit bump in the predictor). This creates an immediate headache: what does a “reasonable” prior look like for log-odds? Is normal(0, 1) too wide? Too narrow? It’s hard to have intuitions about log-odds because we don’t naturally think that way. How do we translate our existed knowledge (or intuitions) about effect sizes into appropriate priors for log-odds coefficients?\nThere’s a clever heuristic that can help us translate our familiar Cohen’s d intuitions into the log-odds world.\n\nA Useful Translation Trick\nSánchez-Meca, Marín-Martínez, and Chacón-Moscoso (2003) developed a relationship between odds ratios and Cohen’s d for meta-analytic contexts:\n\\[d = log(OR) \\times \\frac{\\sqrt{3}}{\\pi}\\]\nRearranging gives us:\n\\[log(OR) = d \\times \\frac{\\pi}{\\sqrt{3}}\\] Because a logistic distribution has variance π² / 3 while a standard normal has variance 1, multiplying a log-odds-ratio by √3 / π (≈ 0.551) rescales it into d. Handy, but only a rough guide—so wield with caution.\nWe can use this as a starting point for prior specification. If we expect mostly small-to-medium effects (d ≈ 0.2-0.5), we can translate those familiar benchmarks into log-odds standard deviations.\n\n\nA Practical Workflow\nHere’s how I use this approach:\nStep 1: Think in Cohen’s d terms\nFor spam detection, most individual features probably have small to medium effects:\n\nSmall effect: d ≈ 0.2\nMedium effect: d ≈ 0.5\nLarge effect: d ≈ 0.8\n\nStep 2: Convert to log-odds standard deviation\nSince we want unbiased estimates, we want our prior to be centered at zero (no effect), therefore we can set the standard deviation of our normal prior using:\n\\[\\sigma_{\\log-odds} = d \\times \\frac{\\pi}{\\sqrt{3}}\\]\nLet me show you how this works in practice:\n\n\n\n\n\n\n\n\n\nSmall effects lead to narrow distributions, which creates stronger regularization. When we expect our predictors to have small effects (d = 0.2), the resulting prior standard deviation of ~0.36 keeps coefficients tightly concentrated around zero. This way we are preventing overfitting by shrinking coefficients toward zero unless the data provides strong evidence otherwise. In contrast, expecting medium effects (d = 0.5) gives us a wider prior (σ ≈ 0.91) that allows coefficients more freedom to deviate from zero.\nThis makes intuitive sense: if we genuinely believe our features have small effects, we should be skeptical of large coefficient estimates and let the prior express that skepticism through increased shrinkage.\nThere you go! We have our priors! Let’s specify our model structure.\n\n\n\nChoosing Predictors\nThe choice of predictors typically depends on your goals—theory testing versus prediction optimization. Here, I’ve chosen predictors for tutorial purposes rather than optimal spam detection. Each one helps illustrate different aspects of Bayesian logistic regression interpretation:\n  is_spam ~ (pc_aggression + pc_incoherence) * \n    (word_count + exclamation_count)\nThese predictors give us different story types to tell:\n\npc_aggression: “How does linguistic hostility signal spam?”\nword_count: “Do spammers prefer short punchy messages or longer sales pitches?”\nexclamation_count: “When do exclamation points become suspicious?”\nInteractions: “How do these patterns combine and depend on each other?”\n\n\n\nBringing It All Together: Fitting the Model in brms\nNow let’s translate our prior intuitions and model structure into actual brms code. This is where everything comes together—our Cohen’s d-derived priors meet our pedagogically-chosen predictors.\nlibrary(brms)\nlibrary(bayestestR)\n\n# Scaling the predictors first\n\nmodel_data &lt;- model_data %&gt;%\n  mutate(\n    across(\n      all_of(c(\"pc_aggression\",\n                    \"word_count\",\n                    \"pc_incoherence\",\n                    \"exclamation_count\")),\n      ~ as.numeric(\n        scale(.x)\n        )\n      )\n    )\n\n\n# Set our priors based on expected small-to-medium effects\nd_expected &lt;- 0.3\nprior_sd &lt;- d_expected * pi / sqrt(3)  # ≈ 0.54\n\n# Define priors for all coefficients\npriors &lt;- c(\n  prior(normal(0, 0.54), class = b)  # All slopes get the same prior\n)\n\n# Fit the model\nspam_model &lt;- brm(        \n  is_spam ~ (pc_aggression + pc_incoherence) * \n            (word_count + exclamation_count),\n  data = model_data,\n  family = bernoulli(),\n  prior = priors,\n  cores = 4,\n  iter = 2000,\n  warmup = 1000,\n  chains = 4,\n  seed = 123\n)\nHere are some basic diagnostic for the model:\n\nPoseterior Predictive CheckR-hatROC\n\n\n\npp_plot\n\n\n\n\n\n\n\n\n\n\n\nrhat_plot\n\n\n\n\n\n\n\n\n\n\n\nroc_plot\n\n\n\n\n\n\n\n\n\n\n\nIt’s time to interpret the model’s effects. But how?\n\n\nHDI+ROPE: A Bayesian Path to Statistical Inference\nTraditional null hypothesis significance testing (NHST) with p-values pushed us into a binary world: an effect is either “significant” or “not significant” based on an arbitrary threshold (typically p &lt; .05). This approach has been criticized extensively—not least because it collapses a continuous measure of evidence into a dichotomous decision. It’s like trying to describe a complex spam pattern with just “suspicious” or “not suspicious”—when in reality, there’s a rich spectrum of possible spam indicators.\nBayesian inference offers a more nuanced perspective through posterior distributions. Instead of a single p-value, we get an entire distribution of plausible parameter values. But this richness creates a new challenge: how do we make practical decisions without being overwhelmed by distributional complexity?\nJohn Kruschke (2014, 2018) developed a powerful framework that provides a principled alternative to traditional significance testing.\n\nThe Highest Density Interval (HDI)\nThe HDI contains a specified percentage of the most probable parameter values, where every value inside the interval has higher probability density than any value outside. Unlike frequentist confidence intervals, the HDI has a direct probabilistic interpretation: given our data and model, there’s an X% probability that the true parameter value lies within the X% HDI (e.g., 95% probability for a 95% HDI).\nWhile Kruschke originally recommended using 95% HDIs (and later suggested 89% as potentially better), we’ll take advantage of the full posterior distribution (100% HDI)—using the complete picture of uncertainty in our spam analysis.\n\n\nThe Region of Practical Equivalence (ROPE)\nThe ROPE is essentially us asking, “What effect is so small that I wouldn’t care about them in practice?” For illustration, we might set our ROPE at ±5%—meaning any feature that changes spam probability by less than 5 percentage points either way is considered practically negligible.\nImportantly, ROPE represents a shift from traditional significance testing to effect size reasoning. Rather than asking “Is there any effect, no matter how small?” (significance testing), we ask “Is the effect large enough to matter?” (effect size evaluation). If adding one exclamation point increases spam probability by 0.01%, that might be statistically detectable with enough data, but no spam filter designer would ever notice or care. The ROPE lets us define this “too small to matter” range—distinguishing between effects that are statistically detectable versus practically meaningful for spam detection. This focus on effect magnitude rather than mere detectability represents a fundamental shift toward more substantive scientific inference.\n\n\nMaking Decisions with HDI+ROPE\nWhen using the full posterior distribution approach, the decision rules are straightforward. Using our 5% example threshold (though other values may be appropriate for different contexts):\n\nReject the null hypothesis if less than 2.5% of the posterior distribution falls within the ROPE. This means we have strong evidence for a practically meaningful effect. The visualization below shows this as a distribution clearly extending beyond our the ROPE boundaries.\nAccept the null hypothesis if more than 97.5% of the posterior distribution falls within the ROPE. This means we have evidence for the practical absence of an effect—the parameter is essentially equivalent to zero in spam detection terms. This appears as a distribution tightly concentrated within the ROPE zone.\nRemain undecided if the percentage falls between these thresholds. The evidence is inconclusive at our current precision level, shown as distributions that substantially span the ROPE boundaries.\n\nThe following visualization demonstrates these decision rules in action, showing four distinct patterns you might encounter when analyzing spam features. Each scenario represents a different relationship between the posterior distribution and our example ±5% ROPE, illustrating how the same statistical framework can yield different conclusions depending on where the evidence falls. Notice how some effects might be statistically detectable (consistent small impacts) yet still fall within our practical equivalence zone—a nuance that traditional p-value approaches often miss.\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing the Spam data with marginaleffects and bayestestR\nAfter this conceptual introduction, how do we actually implement HDI+ROPE in practice? We’ll harness two powerful R packages that make this analysis both rigorous and accessible.\n\nWhy marginaleffects for Logistic Regression?\nThis tutorial happened to be already too long, so we won’t dive deeply into why marginaleffects is transformative for logistic regression analysis. But in brief, this package solves several critical pain points:\n\nMeaningful effect sizes: Rather than wrestling with log-odds coefficients that nobody intuitively understands, marginaleffects automatically translates everything to the probability scale—telling us how spam probability actually changes, not just abstract log-odds ratios. We can’t escape logistic regression’s inherent property where effects vary across different regions of the S-curve, but we can measure and report these varying effects in a transparent, interpretable way.\nInteraction interpretation made simple: Our model includes four interactions ((A+B)*(C+D)), which would traditionally require careful algebra and chain rule calculations. The package handles all the calculus behind the scenes.\nUncertainty propagation done right: Every estimate comes with properly computed standard errors that account for the non-linear transformations inherent in logistic regression.\nSeamless Bayesian integration: The package works identically with brms models as with frequentist ones, automatically extracting posterior draws when needed. This means our workflow remains consistent whether we’re computing average marginal effects or testing complex hypotheses.\n\nFor a comprehensive exploration of these capabilities, Andrew Heiss provides an excellent deep dive here.\n\n\nWhy bayestestR for ROPE?\nWhile marginaleffects handles the effect computation, bayestestR provides the decision-making framework. It offers battle-tested functions to calculate the proportion of the posterior distribution falling within our ROPE. This seamless integration means we can move from posterior distributions to practical decisions without manual probability calculations or custom functions.\n\n\nMaking Sense of the Results\nLet’s see this powerful combination in action by computing the average marginal effects for each predictor. These tell us how much each feature changes the probability of spam classification, averaged across all observations in our data:\n\nlibrary(marginaleffects)\nlibrary(tidybayes)\nlibrary(ggdist)\n\nrope = c(-0.05,0.05)\nci  = 1.0\n\nmain_effects &lt;- avg_slopes(spam_model,type = \"response\")\n\nbayestestR::ci(main_effects,ci = ci, method = \"HDI\")\n\nHighest Density Interval\n\nterm              | contrast |       100% HDI\n---------------------------------------------\nexclamation_count |    dY/dX | [ 0.00,  0.05]\npc_aggression     |    dY/dX | [-0.07, -0.01]\npc_incoherence    |    dY/dX | [-0.19, -0.14]\nword_count        |    dY/dX | [ 0.05,  0.11]\n\n\nSince we standardized all predictors before analysis, these effects represent the impact of a one standard deviation change in each feature.\nLooking solely on the HDIs, the first thing that jumps out is that all four predictors main effects show “statistically significant” effects in the traditional sense—not a single HDI includes zero.\nIn the old world of p-values, we’d declare victory—four significant predictors! Pop the champagne! But wait… let’s look at what happens when we apply our HDI+ROPE examination:\n\nrope(main_effects ,range = rope,ci = ci)\n\n# Proportion of samples inside the ROPE [-0.05, 0.05]:\n\nterm              | contrast | inside ROPE\n------------------------------------------\nexclamation_count |    dY/dX |     99.92 %\npc_aggression     |    dY/dX |     86.83 %\npc_incoherence    |    dY/dX |      0.00 %\nword_count        |    dY/dX |      0.00 %\n\n\nRemember, we set our ROPE at ±5% (e.g., ±0.05 probability points)—any effect smaller than a 5 percentage point change in spam probability is too small to matter for practical spam filtering. Now watch what happens:\nTwo of our four “statistically significant” predictors fall predominantly within the ROPE:\n\nexclamation_count: 99.92% of the posterior is inside ROPE - supporting the null hypothesis\npc_aggression: 86.83% of the posterior is inside ROPE - which is an inconclusive result that leans toward the null hypothesis.\n\nThe other two predictors are showing practical significance:\n\npc_incoherence: 0% inside the ROPE (averaging around -16.5%)\nword_count: 0% inside the ROPE (averaging around +8%)\n\nThis is exactly why HDI+ROPE analysis is so powerful. Traditional significance testing would have given us four “significant” results without distinguishing their practical importance. Our Bayesian approach reveals which effects actually matter: linguistic incoherence strongly signals legitimate messages (reducing spam probability by about 16.5%), while longer messages are more likely to be spam (increasing probability by about 8%).\n\n\n\n\n\n\n\n\n\nThe visualization brings this distinction to life. The gray distributions (exclamation_count and pc_aggression) cluster around zero, mostly contained within our ROPE boundaries—statistically detectable but practically negligible. In contrast, both pc_incoherence and word_count distributions (in blue) extend well beyond the ROPE. The incoherence effect is our strongest predictor, while the positive word_count effect suggests that spammers tend to write longer messages—perhaps needing more words to spin their tales of exotic princes or miracle cures.\n\nExploring Interactions\nSo far, we’ve examined how each feature independently affects spam probability. But real-world spam detection is more nuanced—the impact of one feature often depends on the context provided by others. With our model specification inspecting interactions, we’re explicitly allowing for these interdependencies.\nWhen working with continuous predictors, interactions create a little twist: the effect of one variable literally changes as we move along the range of another variable. Think of it this way—the impact of aggressive language on spam probability might be minimal in very short messages (where there’s little room for aggression to manifest) but could become substantial in longer messages (where sustained aggressive tone becomes more apparent).\nTo capture these shifting relationships, we need to examine how effects vary across different contexts. The avg_slopes function with datagrid helps us do exactly this—it calculates the average effect across specified points along the moderating variable’s distribution. This gives us a single summary measure of how strong the interaction effect is overall.\nLet me demonstrate with our four interaction pairs:\n\n# Interaction 1: How does pc_incoherence's effect vary with message length?\nincoherence_by_wordcount &lt;- avg_slopes(\n    spam_model,\n    variables = \"pc_incoherence\",\n    newdata   = datagrid(\n        word_count = quantile(model_data$word_count, \n                            probs = c(.05, .25, .50, .75, .95))\n    ),\n    type      = \"response\"\n)\n\n# Interaction 2: How pc_incoherence's effect changes with exclamation count\nincoherence_by_exclamation &lt;- avg_slopes(\n    spam_model,\n    variables = \"pc_incoherence\",\n    newdata   = datagrid(\n        exclamation_count = quantile(model_data$exclamation_count, \n                                   probs = c(.05, .25, .50, .75, .95))\n    ),\n    type      = \"response\"\n)\n\n# Interaction 3: How pc_aggression's effect changes with exclamation count\naggression_by_exclamation &lt;- avg_slopes(\n    spam_model,\n    variables = \"pc_aggression\",\n    newdata   = datagrid(\n        exclamation_count = quantile(model_data$exclamation_count, \n                                   probs = c(.05, .25, .50, .75, .95))\n    ),\n    type      = \"response\"\n)\n\n# Interaction 4: How pc_aggression's effect changes with word count\naggression_by_wordcount &lt;- avg_slopes(\n    spam_model,\n    variables = \"pc_aggression\",\n    newdata   = datagrid(\n        word_count = quantile(model_data$word_count, \n                            probs = c(.05, .25, .50, .75, .95))\n    ),\n    type      = \"response\"\n)\n\nbind_rows(\nrope(incoherence_by_wordcount, range = rope, ci = ci) %&gt;%\n  mutate(Parameter = \"incoherence × word_count\"),\n\nrope(incoherence_by_exclamation, range = rope, ci = ci) %&gt;%\n  mutate(Parameter = \"incoherence × exclamation\"),\n\nrope(aggression_by_exclamation, range = rope, ci = ci) %&gt;%\n  mutate(Parameter = \"aggression × exclamation\"),\n\nrope(aggression_by_wordcount, range = rope, ci = ci) %&gt;%\n  mutate(Parameter = \"aggression × word_count\")\n)\n\n# Proportion of samples inside the ROPE [-0.05, 0.05]:\n\nParameter                 | inside ROPE\n---------------------------------------\nincoherence × word_count  |      0.00 %\nincoherence × exclamation |      0.00 %\naggression × exclamation  |      9.93 %\naggression × word_count   |     63.48 %\n\n\nLooking at these interaction results, distinct patterns emerge across our four combinations. The two interactions involving linguistic incoherence show substantial effects that completely escape our ROPE boundaries (0% inside). The aggression interactions tell a more varied story—the aggression × exclamation interaction is shy from a meaningful practical importance with only 9.93% falling within the ROPE, while the aggression × word_count interaction is non decisive at 63.48% inside the ROPE.\nThink about what this means in practical terms. When a message scores high on incoherence—perhaps it’s a hastily typed personal message or an auto-generated notification with templated chunks—it’s substantially less likely to be spam. This effect averages around -20% across different message lengths and punctuation patterns. The consistency is striking: whether it’s a brief “Running late, see u soon!” or a longer rambling message full of typos and incomplete thoughts, linguistic incoherence signals authenticity rather than commercial intent.\nThese patterns reveal how spam characteristics combine in practice. The incoherence effect remains robust across different contexts—whether messages are short or long, heavily punctuated or not, linguistic incoherence consistently signals legitimate communication with effects around -17% to -19%. This stability suggests that real human messiness in texting transcends other message characteristics and trustworthiness.\nThe aggression × exclamation interaction deserves special attention. While aggressive language alone showed negligible main effects, its combination with heavy exclamation mark usage could create a meaningful spam signal. This makes intuitive sense: the pattern of aggressive tone plus excessive punctuation (“URGENT!!! CLAIM NOW!!!”) represents a classic spam signature that our model successfully identifies. The interaction captures something neither component could detect alone—the multiplicative effect of multiple spam tactics used together.\nHere’s the visualization showing all four interaction effects:"
  },
  {
    "objectID": "posts/Logistict.ROPE/Beyond!!!.html#wrapping-up-a-new-lens-for-binary-classification",
    "href": "posts/Logistict.ROPE/Beyond!!!.html#wrapping-up-a-new-lens-for-binary-classification",
    "title": "Beyond the Exclamation Points!!!",
    "section": "Wrapping Up: A New Lens for Binary Classification",
    "text": "Wrapping Up: A New Lens for Binary Classification\nWe started this journey frustrated with log-odds coefficients that nobody could interpret. Through the combination of Bayesian inference, marginal effects, and HDI+ROPE analysis, we’ve transformed an opaque logistic regression into a story that is easier to understand.\nBut the real victory here isn’t just about spam detection. It’s about the analytical framework we’ve demonstrated. We tackled one of Bayesian analysis’s most intimidating challenges—setting priors for log-odds coefficients—by developing a practical heuristic that translates familiar Cohen’s d effect sizes into appropriate prior distributions. By combining brms for robust Bayesian estimation, marginaleffects for interpretable effect sizes, and bayestestR for principled decision-making, we’ve created a workflow that turns statistical significance into practical insight. No more explaining odds ratios to confused stakeholders. No more pretending that p &lt; 0.05 means something matters in the real world.\nThe HDI+ROPE approach deserves special emphasis. It elegantly solves the fundamental tension in applied statistics: distinguishing between effects we can detect and effects that actually matter. In our analysis, we found multiple “statistically significant” predictors that were practically useless—a distinction that traditional methods would have missed entirely.\nFor practitioners working with binary outcomes—whether in spam detection, medical diagnosis, customer churn, or any other domain—this framework offers a path forward. Set your ROPE based on domain expertise, specify your priors using the Cohen’s d translation trick, fit your model with confidence, and let the posterior distributions tell you not just what’s real, but what’s worth caring about. The result is statistical analysis that speaks the language of practical decision-making, turning the notorious complexity of logistic regression into insights that drive action.\n\n\n\n\n\n\nSession Info\n\n\n\n\n\n\nsessioninfo::session_info()\n\n─ Session info ─────────────────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       macOS Ventura 13.3.1\n system   x86_64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Asia/Jerusalem\n date     2025-07-13\n pandoc   3.5 @ /usr/local/bin/ (via rmarkdown)\n quarto   1.6.40 @ /usr/local/bin/quarto\n\n─ Packages ─────────────────────────────────────────────────────────────────────────────\n package         * version    date (UTC) lib source\n abind             1.4-8      2024-09-12 [1] CRAN (R 4.4.1)\n arrayhelpers      1.1-0      2020-02-04 [1] CRAN (R 4.4.0)\n backports         1.5.0      2024-05-23 [1] CRAN (R 4.4.0)\n bayesplot       * 1.13.0     2025-06-18 [1] CRAN (R 4.4.1)\n bayestestR      * 0.16.0     2025-05-20 [1] CRAN (R 4.4.1)\n bridgesampling    1.1-2      2021-04-16 [1] CRAN (R 4.4.0)\n brms            * 2.22.0     2024-09-23 [1] CRAN (R 4.4.1)\n Brobdingnag       1.2-9      2022-10-19 [1] CRAN (R 4.4.0)\n checkmate         2.3.2      2024-07-29 [1] CRAN (R 4.4.0)\n class             7.3-23     2025-01-01 [1] CRAN (R 4.4.1)\n classInt          0.4-11     2025-01-08 [1] CRAN (R 4.4.1)\n cli               3.6.5      2025-04-23 [1] CRAN (R 4.4.1)\n cluster           2.1.8      2024-12-11 [1] CRAN (R 4.4.1)\n cmdstanr          0.8.1      2025-02-01 [1] https://stan-dev.r-universe.dev (R 4.4.2)\n coda              0.19-4.1   2024-01-31 [1] CRAN (R 4.4.0)\n codetools         0.2-20     2024-03-31 [1] CRAN (R 4.4.2)\n collapse          2.0.19     2025-01-09 [1] CRAN (R 4.4.1)\n colorspace        2.1-1      2024-07-26 [1] CRAN (R 4.4.0)\n crayon            1.5.3      2024-06-20 [1] CRAN (R 4.4.0)\n curl              6.4.0      2025-06-22 [1] CRAN (R 4.4.1)\n data.table        1.17.6     2025-06-17 [1] CRAN (R 4.4.1)\n DBI               1.2.3      2024-06-02 [1] CRAN (R 4.4.0)\n digest            0.6.37     2024-08-19 [1] CRAN (R 4.4.1)\n distributional    0.5.0      2024-09-17 [1] CRAN (R 4.4.1)\n dplyr           * 1.1.4      2023-11-17 [1] CRAN (R 4.4.0)\n DT                0.33       2024-04-04 [1] CRAN (R 4.4.0)\n e1071             1.7-16     2024-09-16 [1] CRAN (R 4.4.1)\n emmeans           1.11.1     2025-05-04 [1] CRAN (R 4.4.1)\n estimability      1.5.1      2024-05-12 [1] CRAN (R 4.4.0)\n evaluate          1.0.3      2025-01-10 [1] CRAN (R 4.4.1)\n FactoMineR      * 2.11       2024-04-20 [1] CRAN (R 4.4.0)\n farver            2.1.2      2024-05-13 [1] CRAN (R 4.4.0)\n fastmap           1.2.0      2024-05-15 [1] CRAN (R 4.4.0)\n flashClust        1.01-2     2012-08-21 [1] CRAN (R 4.4.0)\n forcats         * 1.0.0      2023-01-29 [1] CRAN (R 4.4.0)\n generics          0.1.4      2025-05-09 [1] CRAN (R 4.4.1)\n gganimate       * 1.0.9      2024-02-27 [1] CRAN (R 4.4.0)\n ggdist          * 3.3.2      2024-03-05 [1] CRAN (R 4.4.0)\n ggplot2         * 3.5.2      2025-04-09 [1] CRAN (R 4.4.1)\n ggrepel           0.9.6      2024-09-07 [1] CRAN (R 4.4.1)\n gifski          * 1.32.0-1   2024-10-13 [1] CRAN (R 4.4.1)\n glue            * 1.8.0      2024-09-30 [1] CRAN (R 4.4.1)\n gridExtra         2.3        2017-09-09 [1] CRAN (R 4.4.0)\n gtable            0.3.6      2024-10-25 [1] CRAN (R 4.4.1)\n hms               1.1.3      2023-03-21 [1] CRAN (R 4.4.0)\n htmltools         0.5.8.1    2024-04-04 [1] CRAN (R 4.4.0)\n htmlwidgets       1.6.4      2023-12-06 [1] CRAN (R 4.4.0)\n inline            0.3.21     2025-01-09 [1] CRAN (R 4.4.1)\n insight           1.3.0      2025-05-20 [1] CRAN (R 4.4.1)\n jsonlite          2.0.0      2025-03-27 [1] CRAN (R 4.4.1)\n KernSmooth        2.23-26    2025-01-01 [1] CRAN (R 4.4.1)\n knitr             1.49       2024-11-08 [1] CRAN (R 4.4.1)\n labeling          0.4.3      2023-08-29 [1] CRAN (R 4.4.0)\n lattice           0.22-6     2024-03-20 [1] CRAN (R 4.4.2)\n leaps             3.2        2024-06-10 [1] CRAN (R 4.4.0)\n lifecycle         1.0.4      2023-11-07 [1] CRAN (R 4.4.0)\n loo               2.8.0      2024-07-03 [1] CRAN (R 4.4.0)\n lpSolve           5.6.23     2024-12-14 [1] CRAN (R 4.4.1)\n lubridate       * 1.9.4      2024-12-08 [1] CRAN (R 4.4.1)\n magrittr          2.0.3      2022-03-30 [1] CRAN (R 4.4.0)\n marginaleffects * 0.28.0     2025-06-25 [1] CRAN (R 4.4.1)\n MASS              7.3-64     2025-01-04 [1] CRAN (R 4.4.1)\n Matrix            1.7-2      2025-01-23 [1] CRAN (R 4.4.1)\n matrixStats       1.5.0      2025-01-07 [1] CRAN (R 4.4.1)\n multcomp          1.4-28     2025-01-29 [1] CRAN (R 4.4.1)\n multcompView      0.1-10     2024-03-08 [1] CRAN (R 4.4.0)\n munsell           0.5.1      2024-04-01 [1] CRAN (R 4.4.0)\n mvtnorm           1.3-3      2025-01-10 [1] CRAN (R 4.4.1)\n nlme              3.1-167    2025-01-27 [1] CRAN (R 4.4.1)\n patchwork       * 1.3.0      2024-09-16 [1] CRAN (R 4.4.1)\n peRspective     * 0.1.1      2025-06-27 [1] Github (favstats/peRspective@4373272)\n pillar            1.10.2     2025-04-05 [1] CRAN (R 4.4.1)\n pkgbuild          1.4.6      2025-01-16 [1] CRAN (R 4.4.1)\n pkgconfig         2.0.3      2019-09-22 [1] CRAN (R 4.4.0)\n plyr              1.8.9      2023-10-02 [1] CRAN (R 4.4.0)\n posterior       * 1.6.0.9000 2025-01-30 [1] https://stan-dev.r-universe.dev (R 4.4.2)\n prettyunits       1.2.0      2023-09-24 [1] CRAN (R 4.4.0)\n pROC            * 1.18.5     2023-11-01 [1] CRAN (R 4.4.0)\n processx          3.8.5      2025-01-08 [1] CRAN (R 4.4.1)\n progress          1.2.3      2023-12-06 [1] CRAN (R 4.4.0)\n proxy             0.4-27     2022-06-09 [1] CRAN (R 4.4.0)\n ps                1.8.1      2024-10-28 [1] CRAN (R 4.4.1)\n purrr           * 1.0.4      2025-02-05 [1] CRAN (R 4.4.1)\n QuickJSR          1.5.1      2025-01-08 [1] CRAN (R 4.4.1)\n R6                2.6.1      2025-02-15 [1] CRAN (R 4.4.1)\n Rcpp            * 1.0.14     2025-01-12 [1] CRAN (R 4.4.1)\n RcppParallel      5.1.10     2025-01-24 [1] CRAN (R 4.4.1)\n readr           * 2.1.5      2024-01-10 [1] CRAN (R 4.4.0)\n reshape2          1.4.4      2020-04-09 [1] CRAN (R 4.4.0)\n rlang             1.1.6      2025-04-11 [1] CRAN (R 4.4.1)\n rmarkdown         2.29       2024-11-04 [1] CRAN (R 4.4.1)\n rstan           * 2.32.6     2024-03-05 [1] CRAN (R 4.4.1)\n rstantools        2.4.0      2024-01-31 [1] CRAN (R 4.4.1)\n rstudioapi        0.17.1     2024-10-22 [1] CRAN (R 4.4.1)\n sandwich          3.1-1      2024-09-15 [1] CRAN (R 4.4.1)\n scales            1.3.0      2023-11-28 [1] CRAN (R 4.4.0)\n scatterplot3d     0.3-44     2023-05-05 [1] CRAN (R 4.4.0)\n sessioninfo       1.2.3      2025-02-05 [1] CRAN (R 4.4.1)\n sf                1.0-19     2024-11-05 [1] CRAN (R 4.4.1)\n StanHeaders     * 2.32.10    2024-07-15 [1] CRAN (R 4.4.1)\n stringi           1.8.7      2025-03-27 [1] CRAN (R 4.4.1)\n stringr         * 1.5.1      2023-11-14 [1] CRAN (R 4.4.0)\n survival          3.8-3      2024-12-17 [1] CRAN (R 4.4.1)\n svUnit            1.0.6      2021-04-19 [1] CRAN (R 4.4.0)\n tensorA           0.36.2.1   2023-12-13 [1] CRAN (R 4.4.0)\n TH.data           1.1-3      2025-01-17 [1] CRAN (R 4.4.1)\n tibble          * 3.3.0      2025-06-08 [1] CRAN (R 4.4.1)\n tidybayes       * 3.0.7      2024-09-15 [1] CRAN (R 4.4.1)\n tidyr           * 1.3.1      2024-01-24 [1] CRAN (R 4.4.0)\n tidyselect        1.2.1      2024-03-11 [1] CRAN (R 4.4.0)\n tidyverse       * 2.0.0      2023-02-22 [1] CRAN (R 4.4.0)\n timechange        0.3.0      2024-01-18 [1] CRAN (R 4.4.0)\n transformr        0.1.5      2024-02-26 [1] CRAN (R 4.4.0)\n tweenr            2.0.3      2024-02-26 [1] CRAN (R 4.4.0)\n tzdb              0.4.0      2023-05-12 [1] CRAN (R 4.4.0)\n units             0.8-5      2023-11-28 [1] CRAN (R 4.4.0)\n V8                6.0.1      2025-02-02 [1] CRAN (R 4.4.1)\n vctrs             0.6.5      2023-12-01 [1] CRAN (R 4.4.0)\n withr             3.0.2      2024-10-28 [1] CRAN (R 4.4.1)\n xfun              0.50       2025-01-07 [1] CRAN (R 4.4.1)\n xtable            1.8-4      2019-04-21 [1] CRAN (R 4.4.1)\n yaml              2.3.10     2024-07-26 [1] CRAN (R 4.4.0)\n zoo               1.8-12     2023-04-13 [1] CRAN (R 4.4.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/library\n * ── Packages attached to the search path.\n\n────────────────────────────────────────────────────────────────────────────────────────"
  }
]