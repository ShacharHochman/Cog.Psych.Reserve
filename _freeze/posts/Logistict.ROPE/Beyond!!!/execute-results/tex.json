{
  "hash": "dab5583a3c401cf8e2b390a435d40ff0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Beyond the Exclamation Points!!!\"\nsubtitle: \"HDI-ROPE for Binary Outcomes: What Makes a Text Message Spam?\"\nauthor: \"Shachar Hochman\"\ndate: \"2025-05-21\"\nimage: linear_to_logistic_persistent.gif\npost-type: article\ncategories: \n  - Bayesian\n  - logistic-regression\n  - hdi-rope\n  - spam-detection\nformat:\n  pdf: default\n  html:\n    toc: true\n    self-contained: true\n    code-summary: \"Show the R code\"\n    code-fold: false\n    fig-align: center\n    math: mathjax\n    fig-cap-location: top    # caption above the figure (optional)\n    dpi: 96                  # crisp but lightweight\n  \n---\n\n\n\n\n\n\n## Why Are We Here?\n\n**Goal**: Demonstrate a clearer approach to interpreting logistic regression using Bayesian methods and HDI-ROPE analysis, illustrated through real-world SMS spam detection.\n\n**Case Study**: Predicting whether an SMS message is spam based on linguistic toxicity patterns (captured through NLP+PCA), message length, and punctuation usage.\n\n**The Bottom Line**: This tutorial shows how Bayesian modeling combined with marginal effects and HDI-ROPE analysis creates a more intuitive workflow for binary outcome analysis—avoiding the notorious \"log-odds\" interpretation problem while tackling the practical challenge of spam detection.\n\n::: callout-caution\n## I make assumptions (too)!\n\nI assume you:\n\n\\- Have basic familiarity with R and the tidyverse.\n\n\\- Understand the fundamentals of regression analysis.\n\n\\- Have encountered logistic regression before and familiar with the interpretation frustration.\n:::\n\n## The Messages Behind the Data: Understanding SMS Spam in Context\n\nImagine receiving a text message: \n> \"URGENT!!! You have WON £1,000,000!!! Reply NOW with your bank details!!!\" \n\nYour brain instantly recognizes this as spam—but how? It's not just excessive exclamation points or too-good-to-be-true offers. There's a complex pattern of linguistic signals distinguishing legitimate messages from spam, which can vary significantly across cultural and linguistic contexts.\n\nThe dataset we're exploring comes from the [ExAIS SMS Spam project](https://www.kaggle.com/datasets/ysfbil/exais-sms-dataset) conducted in Nigeria, featuring 5,240 SMS messages (2,350 spam, 2,890 ham) collected from university community members aged 20–50.\n\nThe data contain the SPAM/HAM (not spam) classification and the text message itself. Using NLP magic, dimensionality reduction, and text analysis (see note below for full code), I extracted some additional features:\n\n-   **is_spam** - Our outcome variable; a binary classification indicating spam or legitimate communication.\n\n-   **pc_aggression & pc_incoherence** - Principal components I extracted from Google's Perspective API toxicity scores capturing sophisticated linguistic patterns:\n\n    +   **pc_aggression:** threatening, toxic, and insulting language patterns..\n\n    +   **pc_incoherence** inflammatory, incoherent, or unsubstantial messages.\n\n-   **word_count** - The total number of words per message (includes squared term to capture non-linear relationships).\n\n-   **exclamation_count** - The number of exclamation points, a simple but telling spam feature.\n\n[Our central question is this]{.underline}: How do linguistic toxicity patterns (captured by PCA components) interact with message characteristics like length and punctuation to identify spam? And more importantly, how can we interpret these relationships meaningfully?\n\n::: {.callout-note collapse=\"true\"}\n## A Note on Advanced NLP Features\n\nBelow is the exact, reproducible pipeline I used to transform raw text into the two tidy principal‑component dials (`pc_aggression`, `pc_incoherence`) you saw in the main post. Everything is wrapped in code‑chunks so you (or your future self) can copy‑paste the whole block into **Quarto/R Markdown** and run it end‑to‑end.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ── 1 · Load required packages ───────────────────────────────────────────\nlibrary(tidyverse)     # data manipulation and pipes\nlibrary(peRspective)   # R client for Google/Jigsaw Perspective API\nlibrary(FactoMineR)    # Principal‑Component Analysis\nlibrary(pROC)          # ROC curves (for later evaluation)\n```\n:::\n\n\n\n\n> **What is *peRspective*?**\\\n> `peRspective` is a thin, tidyverse‑friendly wrapper around the [Perspective API](https://developers.perspectiveapi.com/). It takes care of batching requests, retrying on rate‑limits, and returning scores as a clean data frame. Before running the chunk below you'll need a free API key (set it once with `Sys.setenv(PERSPECTIVE_API_KEY = \"<your‑key>\")`).\n\n### Step 1 -- Obtain linguistic scores from the Perspective API\n\nEach message is sent to the API, which returns up to nine probabilities indicating the presence of attributes such as *toxicity*, *threat*, or *incoherence*.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# ⚠️  Disabled by default to avoid accidental quota use.\nperspective_scores <- dataset_clean %>%\n  prsp_stream(\n    text        = message,          # column containing the SMS text\n    text_id     = text_id,          # unique identifier for safe joins\n    score_model = c(\"THREAT\", \"TOXICITY\", \"INSULT\", \"SPAM\",\n                    \"INFLAMMATORY\", \"INCOHERENT\", \"UNSUBSTANTIAL\",\n                    \"FLIRTATION\", \"PROFANITY\"),\n    safe_output = TRUE,             # masks content the API flags as unsafe\n    verbose     = TRUE)             # progress messages\n```\n:::\n\n\n\n\n> **Development tip:**\\\n> When iterating on downstream scripts, save the scores once (`write_csv()`) and reload them to avoid repeated network calls:\n>\n> ::: {.cell layout-align=\"center\"}\n> \n> ```{.r .cell-code}\n> perspective_scores <- read_csv(\"~/perspective_scores_saved.csv\")\n> ```\n> :::\n\n### Step 2 -- Merge, clean, and prepare for PCA\n\nOccasionally a request fails or returns `NA`. We drop the few problematic rows and replace any missing attribute scores with zeros so PCA receives a complete numeric matrix.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nscored_data <- dataset_clean %>%\n  left_join(perspective_scores, by = \"text_id\") %>%\n  filter(!(has_error = (!is.na(error) & error != \"No Error\"))) %>%\n  mutate(across(THREAT:PROFANITY, ~replace_na(.x, 0))) %>%\n  select(-SPAM, -FLIRTATION)   # remove two attributes found redundant here\n```\n:::\n\n\n\n\n### Step 3 -- Reduce nine correlated scores to two principal components\n\nPrincipal‑Component Analysis (PCA) rotates the nine‑dimensional attribute space until the first component captures the largest systematic variation, the second captures the next‑largest, and so on.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca_result <- PCA(scored_data %>% select(THREAT:PROFANITY),\n                  ncp   = 3,    # keep three components for inspection\n                  graph = FALSE)\n\npca_scores <- as_tibble(pca_result$ind$coord) %>%\n  set_names(c(\"pc_aggression\",     # roughly: threat + toxicity + insult\n              \"pc_inflammatory\",   # moderate mix (not used later)\n              \"pc_incoherence\")) %>%   # roughly: incoherent + unsubstantial\n  mutate(text_id = scored_data$text_id)\n```\n:::\n\n\n\n\n-   **`pc_aggression`**   ranges from *neutral tone* to *overt hostility*.\n-   **`pc_incoherence`** tracks the continuum from *cohesive message* to *nonsensical or fragmentary text*.\n\nThese two components retain ≈ 72 % of the total variance in the original nine attributes, providing a compact yet informative description of each message.\n\n### Step 4 -- Assemble the modelling table\n\nFinally, we merge the PCA scores back with the pre‑computed surface features (`word_count`, `exclamation_count`, etc.) so the eventual model can consider both **linguistic tone** and **formatting cues**.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_data <- scored_data %>%\n  select(-THREAT:-PROFANITY) %>%\n  left_join(pca_scores, by = \"text_id\")\n```\n:::\n\n\n\n\nAt this point `model_data` is a tidy, analysis‑ready data frame: one row per SMS, interpretable columns for tone and structure, and no missing values to trip up downstream methods.\n\n\n\n\n\n\n\n\n:::\n\n## The Interpretation Challenge: <br> Why Binary Outcomes Are Tricky\n\nLogistic regression is the go-to tool when predicting binary outcomes—spam vs. legitimate, click vs. no-click, win vs. loss. But before we dive into logistic regression, let's understand why ordinary linear regression fails with binary data:\n\n### Why a Simple Straight Line Fails\n\nSuppose we try ordinary linear regression for the binary dependent variable:\n\n$$\n\\text{spam} \\;=\\; \\beta_0 \\;+\\; \\beta_1 \\times \\text{aggression}\n                 \\;+\\; \\beta_2 \\times \\text{word count} \\;+\\; \\dots\n$$\n\nTwo big things can go wrong:\n\n1.  [Impossible predictions]{.underline}\\\n    A straight line can spit out 1.2 or --0.3, but probabilities can never exceed 1 or dip below 0.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-2-1.pdf){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n2.  [Uneven \"noise\" (Heteroskedasticity)]{.underline}\n\nLinear regression assumes that prediction errors are both normally distributed and have constant variance across all predicted values. Binary outcomes violate both assumptions.\n\n**The problem:** With a 0/1 outcome, the prediction error depends entirely on the predicted probability:\n\n-   If the true label is **0** and we predict probability `p`, the error is `-p`\n-   If the true label is **1** and we predict probability `p`, the error is `1-p`\n\n**Why this matters:** The magnitude of possible errors varies dramatically with our predicted probability:\n\n-   When `p ≈ 0.50`, errors can be as large as ±0.50\n-   When `p ≈ 0.95`, errors shrink to just ±0.05\n\nThis uneven reliability violates linear regression's core assumption of constant error variance, making the model's confidence intervals and p-values unreliable across different probability ranges.\n\n**The consequence:** Since ordinary linear regression assumes constant error variance, its confidence intervals and p-values become unreliable when applied to binary data. The model \"thinks\" it's more precise than it actually is in some regions and less precise in others.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nThese fundamental problems mean we need a different approach---one that respects probability boundaries and properly handles binary data's quirks. This is where logistic regression shines.\n\n### How Logistic Regression Fixes It\n\nLogistic regression places the linear combination inside a curve that automatically constrains predictions between \\[0, 1\\]:\n\n$$p(\\text{spam}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 \\times \\text{aggression} + \\beta_2 \\times \\text{word count} + ...)}}$$\n\nInside the parentheses is still a plain-vanilla linear combination; the *logistic* curve just keeps the final number honest.\n\n**But here's the rub:** While this S-shaped curve solves our boundary problem, it creates a new headache. The relationship between our predictors and the probability is now curved---a one-unit increase in \"aggression\" might bump spam probability from 10% to 15% in one region, but only from 90% to 91% in another. This makes coefficients nearly impossible to interpret and statistical inference a nightmare.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-2.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-3.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-4.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-5.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-6.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-7.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-8.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-9.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-10.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-11.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-12.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-13.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-14.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-15.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-16.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-17.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-18.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-19.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-20.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-21.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-22.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-23.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-24.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-25.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-26.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-27.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-28.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-29.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-30.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-31.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-32.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-33.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-34.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-35.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-36.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-37.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-38.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-39.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-40.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-41.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-42.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-43.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-44.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-45.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-46.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-47.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-48.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-49.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-50.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-51.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-52.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-53.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-54.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-55.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-56.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-57.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-58.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-59.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-60.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-61.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-62.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-63.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-64.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-65.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-66.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-67.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-68.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-69.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-70.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-71.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-72.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-73.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-74.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-75.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-76.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-77.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-78.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-79.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-80.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-81.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-82.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-83.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-84.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-85.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-86.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-87.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-88.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-89.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-90.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-91.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-92.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-93.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-94.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-95.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-96.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-97.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-98.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-99.pdf){fig-align='center' width=80%}\n:::\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-4-100.pdf){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n### A Peek Behind the Curtain: Log-Odds\n\nLogistic regression works by predicting \"log-odds,\" a transformed measure that enables linear relationships. However, this transformation complicates interpretation for practical audiences unfamiliar with odds.\n\nThink of it as changing measuring units---like converting temperature from Celsius to Fahrenheit, but for probabilities:\n\n-   **Odds** re-phrase probability: *30% chance of spam is equal in* *odds = 0.3 / 0.7 ≈ 0.43*.\n\n-   **Log-odds** take those odds and apply a logarithm. This stretches the probability scale: 0% becomes negative infinity, 100% becomes positive infinity, and everything else spreads out smoothly in between.\n\nOn this stretched-out log-odds scale, we can finally draw our straight line (the right panel):\n\n$$\n\\log\\!\\Bigl(\\tfrac{p}{1-p}\\Bigr)\n  \\;=\\;\n  \\beta_0 \\;+\\; \\beta_1 \\times \\text{aggression}\n           \\;+\\; \\beta_2 \\times \\text{word count}\n           \\;+\\; \\dots\n$$\n\nThe left panel shows the jittered 0/1 spam labels (grey points) alongside the model's predicted probability\\\n$\\displaystyle \\hat p = p(\\text{spam}=1)$, plotted as a solid blue curve on $[0,1]$.\\\nThe right panel displays the same model on the log-odds scale with a dashed red line for\\\n$\\displaystyle \\operatorname{logit}(\\hat p) = \\ln\\!\\bigl(\\tfrac{\\hat p}{1-\\hat p}\\bigr)$, which straightens the S-shaped curve.\\\nIn both panels, grey arrows trace one example predictor value and illustrate how it maps from the probability curve to the straight line in log-odds space.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/logit-curtain-1.pdf){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n## The Interpretation Problem\n\nImagine explaining the model to a spam-filter engineer or product manager:\n\n> \"A one-unit increase in the *aggression* principal component raises the **log-odds** of spam by 0.85.\"\n\nCue blank stares.\\\nSo we translate to **odds ratios**:\n\n> \"Each unit increase in *aggression* multiplies the odds of spam by **2.33** (e^0.85^).\"\n\nStill murky! Even seasoned analysts struggle with odds because humans naturally think in **probabilities**, not odds. Add interaction terms (e.g., how aggression's effect changes with message length) and interpretation gets even thornier.\n\nI'm going to put in **tremendous effort to cut through that fog**---re-expressing results *exclusively* on the familiar 0 %--100 % probability scale, and showing you practical tricks to keep interpretations clear in Bayesian statistics.\n\n::: {.callout-note collapse=\"true\"}\n## But Wait! Why Bayesian?\n\nThe `marginaleffects` package beautifully transforms model results into interpretable probability statements. More on that later. I will not stop here but also combine it with Bayesian estimation to create an even more powerful analytical framework.\n\nBayesian methods solve several technical problems that plague binary outcome models:\n\n### Complete Separation Issues\n\n**The Problem**: Complete separation occurs when a predictor perfectly divides outcome categories. Imagine if every message with more than 10 exclamation points was spam while no message with fewer was---traditional maximum likelihood estimation would produce infinite coefficient estimates.\n\n**The Bayesian Solution**: Priors act as natural regularizers, keeping estimates finite and meaningful even in extreme cases. Instead of model failure, we get sensible uncertainty quantification around our estimates. This is particularly important in spam detection where new tactics constantly emerge, potentially creating separation in specific feature combinations.\n\n### Robust Computation\n\n**The Problem**: Our model includes interactions between PCA components and text features---these complex relationships often cause convergence failures in traditional frameworks. Researchers are forced to simplify their models, potentially missing important patterns in how spam characteristics combine.\n\n**The Bayesian Solution**: Modern implementations like `brms` handle complex model structures that would break optimization-based methods, letting us build models that match our theoretical questions rather than computational constraints.\n\nThis Bayesian foundation, combined with `marginaleffects` for interpretation, gives us the best of both worlds: robust estimation and intuitive communication of results.\n:::\n\n## Making Bayesian Binary Models Practical: From Priors to Inference\n\nThere are two main obstacles that often discourage researchers from adopting Bayesian methods: choosing appropriate **priors** and interpreting **inference** from posterior distributions. My goal here is to show that both are surprisingly straightforward for logistic hierarchical models---especially when we combine the right tools.\n\nLet's build this model step by step, starting with prior specification:\n\n### The Prior Specification Problem\n\nWhen you fit a Bayesian logistic model in `brms`, your regression coefficients live on the **log-odds scale**. This creates an immediate headache: what does a \"reasonable\" prior look like for log-odds? Is `normal(0, 1)` too wide? Too narrow? It's hard to have intuitions about log-odds because we don't naturally think that way. How do we translate our existed knowledge (or intuitions) about effect sizes into appropriate priors for log-odds coefficients?\n\nThere's a clever **heuristic** that can help us translate our familiar Cohen's *d* intuitions into the log-odds world.\n\n#### A Useful Translation Trick\n\nSánchez-Meca, Marín-Martínez, and Chacón-Moscoso (2003) developed a relationship between odds ratios and Cohen's *d* for meta-analytic contexts:\n\n$$d = \\log-odds \\times \\frac{\\sqrt{3}}{\\pi}$$\n\nRearranging gives us:\n\n$$\\log-odds = d \\times \\frac{\\pi}{\\sqrt{3}}$$\n\nWe can use this as a **starting point** for prior specification. If we expect mostly small-to-medium effects (*d* ≈ 0.2-0.5), we can translate those familiar benchmarks into log-odds standard deviations.\n\n*Important caveat:* This conversion was developed for meta-analytic contexts and assumes normally distributed outcomes. We're using it as a rough heuristic for prior specification, not as a precise theoretical relationship.\n\n#### A Practical Workflow\n\nHere's how I use this approach:\n\n**Step 1: Think in Cohen's *d* terms**\n\nFor spam detection, most individual features probably have small to medium effects:\n\n-   Small effect: *d* ≈ 0.2\n\n-   Medium effect: *d* ≈ 0.5\n\n-   Large effect: *d* ≈ 0.8\n\n**Step 2: Convert to log-odds standard deviation**\n\nSince we want unbiased estimates, we want our prior to be centered at zero (no effect), therefore we can set the standard deviation of our normal prior using:\n\n$$\\sigma_{\\log-odds} = d \\times \\frac{\\pi}{\\sqrt{3}}$$\n\nLet me show you how this works in practice:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/prior-setup-1.pdf){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nSmall effects lead to narrow distributions, which creates stronger regularization. When we expect our predictors to have small effects (d = 0.2), the resulting prior standard deviation of \\~0.36 keeps coefficients tightly concentrated around zero. This way we are preventing overfitting by shrinking coefficients toward zero unless the data provides strong evidence otherwise. In contrast, expecting medium effects (d = 0.5) gives us a wider prior (σ ≈ 0.91) that allows coefficients more freedom to deviate from zero.\n\nThis makes intuitive sense: if we genuinely believe our features have small effects, we should be skeptical of large coefficient estimates and let the prior express that skepticism through increased shrinkage.\n\nThere you go! We have our priors! Let's specify our model structure.\n\n### Choosing Predictors\n\nThe choice of predictors typically depends on your goals---theory testing versus prediction optimization. Here, I've chosen predictors for tutorial purposes rather than optimal spam detection. Each one helps illustrate different aspects of Bayesian logistic regression interpretation:\n\n``` r\n  is_spam ~ (scale(pc_aggression) + scale(pc_incoherence)) * \n    (scale(word_count) + scale(exclamation_count))\n```\n\nThese predictors give us different story types to tell:\n\n-   **pc_aggression**: \"How does linguistic hostility signal spam?\"\n\n-   **word_count**: \"Do spammers prefer short punchy messages or longer sales pitches?\"\n\n-   **exclamation_count**: \"When do exclamation points become suspicious?\"\n\n-   **Interactions**: \"How do these patterns combine and depend on each other?\"\n\n### Bringing It All Together: Fitting the Model in brms\n\nNow let's translate our prior intuitions and model structure into actual `brms` code. This is where everything comes together---our Cohen's d-derived priors meet our pedagogically-chosen predictors.\n\n``` r\nlibrary(brms)\nlibrary(bayestestR)\n\noptions(contrasts = c(\"contr.equalprior\", \"contr.poly\"))\n\n# Set our priors based on expected small-to-medium effects\nd_expected <- 0.3\nprior_sd <- d_expected * pi / sqrt(3)  # ≈ 0.54\n\n# Define priors for all coefficients\npriors <- c(\n  prior(normal(0, 0.54), class = b)  # All slopes get the same prior\n)\n\n# Fit the model\nspam_model <- brm(        \n  is_spam ~ (scale(pc_aggression) + scale(pc_incoherence)) * \n            (scale(word_count) + scale(exclamation_count)),\n  data = model_data,\n  family = bernoulli(),\n  prior = priors,\n  cores = 4,\n  iter = 2000,\n  warmup = 1000,\n  chains = 4,\n  seed = 123\n)\n```\n\n\n\n\n\n\n\n\n\n\n\nHere are some basic diagnostic for the model:\n\n::: panel-tabset\n## Poseterior Predictive Check\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npp_plot\n```\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n## R-hat\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrhat_plot\n```\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n\n## ROC\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nroc_plot\n```\n\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-align='center' fig-pos='H' width=80%}\n:::\n:::\n\n\n\n:::\n\nIt's time to interpret the model's effects\n\n### HDI+ROPE: A Bayesian Path to Statistical Inference\n\nTraditional null hypothesis significance testing (NHST) with p-values pushed us into a binary world: an effect is either \"significant\" or \"not significant\" based on an arbitrary threshold (typically p \\< .05). This approach has been criticized extensively---not least because it collapses a continuous measure of evidence into a dichotomous decision. It's like trying to describe a complex spam pattern with just \"suspicious\" or \"not suspicious\"---when in reality, there's a rich spectrum of possible spam indicators.\n\nBayesian inference offers a more nuanced perspective through **posterior distributions**. Instead of a single p-value, we get an entire distribution of plausible parameter values. But this richness creates a new challenge: how do we make practical decisions without being overwhelmed by distributional complexity?\n\nEnter the **HDI+ROPE** decision rule---a powerful framework developed and championed by John Kruschke ([2014](https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884), [2018](https://link.springer.com/article/10.3758/s13423-017-1272-1)) that provides a principled alternative to traditional significance testing while preserving the nuance of Bayesian inference.\n\n#### The Highest Density Interval (HDI)\n\nThe HDI contains a specified percentage of the most probable parameter values, where every value inside the interval has higher probability density than any value outside. Unlike frequentist confidence intervals, the HDI has a direct probabilistic interpretation: given our data and model, there's an X% probability that the true parameter value lies within the X% HDI (e.g., 95% probability for a 95% HDI).\n\nWhile Kruschke originally recommended using 95% HDIs (and later suggested 89% as potentially better), we'll take advantage of the full posterior distribution (100% HDI)---using the complete picture of uncertainty in our spam analysis.\n\n#### The Region of Practical Equivalence (ROPE)\n\nThe ROPE is essentially us asking, \"What effect is so small that I wouldn't care about them in practice?\" For illustration, we might set our ROPE at **±5%**---meaning any feature that changes spam probability by less than 5 percentage points either way is considered practically negligible.\n\nImportantly, ROPE represents a shift from traditional significance testing to **effect size reasoning**. Rather than asking \"Is there any effect, no matter how small?\" (significance testing), we ask \"Is the effect large enough to matter?\" (effect size evaluation). If adding one exclamation point increases spam probability by 0.01%, that might be statistically detectable with enough data, but no spam filter designer would ever notice or care. The ROPE lets us define this \"too small to matter\" range---distinguishing between effects that are statistically detectable versus practically meaningful for spam detection. This focus on effect magnitude rather than mere detectability represents a fundamental shift toward more substantive scientific inference.\n\n#### Making Decisions with HDI+ROPE\n\nWhen using the full posterior distribution approach, the decision rules are straightforward. Using our 5% example threshold (though other values may be appropriate for different contexts):\n\n1.  **Reject the null hypothesis** if less than 2.5% of the posterior distribution falls within the ROPE. This means we have strong evidence for a practically meaningful effect. The visualization below shows this as a distribution clearly extending beyond our the ROPE boundaries.\n\n2.  **Accept the null hypothesis** if more than 97.5% of the posterior distribution falls within the ROPE. This means we have evidence for the practical absence of an effect---the parameter is essentially equivalent to zero in spam detection terms. This appears as a distribution tightly concentrated within the ROPE zone.\n\n3.  **Remain undecided** if the percentage falls between these thresholds. The evidence is inconclusive at our current precision level, shown as distributions that substantially span the ROPE boundaries.\n\nThe following visualization demonstrates these decision rules in action, showing four distinct patterns you might encounter when analyzing spam features. Each scenario represents a different relationship between the posterior distribution and our example ±5% ROPE, illustrating how the same statistical framework can yield different conclusions depending on where the evidence falls. Notice how some effects might be statistically detectable (consistent small impacts) yet still fall within our practical equivalence zone---a nuance that traditional p-value approaches often miss.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-align='center' width=100%}\n:::\n:::\n\n\n\n\n### Analysing the Spam data with `marginaleffects` and `bayestestR`\n\nAfter this conceptual introduction, how do we actually implement HDI+ROPE in practice? We'll harness two powerful R packages that make this analysis both rigorous and accessible.\n\n#### Why `marginaleffects` for Logistic Regression? \n\nThis tutorial happened to be already too long, so we won't dive deeply into why `marginaleffects` is transformative for logistic regression analysis. But in brief, this package solves several critical pain points:\n\n-   **Meaningful effect sizes**: Rather than wrestling with log-odds coefficients that nobody intuitively understands, `marginaleffects` automatically translates everything to the probability scale---telling us how spam probability actually changes, not just abstract log-odds ratios. We can't escape logistic regression's inherent property where effects vary across different regions of the S-curve, but we can measure and report these varying effects in a transparent, interpretable way.\n\n-   **Interaction interpretation made simple:** Our model includes four interactions ((A+B)\\*(C+D)), which would traditionally require careful algebra and chain rule calculations. The package handles all the calculus behind the scenes.\n\n-   **Uncertainty propagation done right:** Every estimate comes with properly computed standard errors that account for the non-linear transformations inherent in logistic regression.\n\n-   **Seamless Bayesian integration:** The package works identically with `brms` models as with frequentist ones, automatically extracting posterior draws when needed. This means our workflow remains consistent whether we're computing average marginal effects or testing complex hypotheses.\n\nFor a comprehensive exploration of these capabilities, Andrew Heiss provides an excellent deep dive at https://www.andrewheiss.com/blog/2022/05/20/marginalia/.\n\n#### Why `bayestestR` for ROPE? \n\nWhile `marginaleffects` handles the effect computation, `bayestestR` provides the decision-making framework. It offers battle-tested functions to calculate the proportion of the posterior distribution falling within our ROPE. This seamless integration means we can move from posterior distributions to practical decisions without manual probability calculations or custom functions. \n\n#### Making Sense of the Results\n\nLet's see this powerful combination in action by computing the average marginal effects for each predictor. These tell us how much each feature changes the probability of spam classification, averaged across all observations in our data:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(marginaleffects)\nlibrary(tidybayes)\nlibrary(ggdist)\n\nrope = c(-0.05,0.05)\nci  = 1.0\n\nmain_effects <- avg_slopes(spam_model,type = \"response\")\n\nbayestestR::ci(main_effects,ci = ci, method = \"HDI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHighest Density Interval\n\nterm              | contrast |       100% HDI\n---------------------------------------------\nexclamation_count |    dY/dX | [ 0.00,  0.06]\npc_aggression     |    dY/dX | [-0.04, -0.01]\npc_incoherence    |    dY/dX | [-0.20, -0.14]\nword_count        |    dY/dX | [ 0.00,  0.01]\n```\n\n\n:::\n:::\n\n\n\n\nLooking solely on the HDIs, the first thing that jumps out is that all four predictors main effects show \"statistically significant\" effects in the traditional sense—not a single HDI includes zero.\n\nIn the old world of p-values, we'd declare victory—four significant predictors! Pop the champagne! But wait... let's look at what happens when we apply our HDI+ROPE examination:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nrope(main_effects ,range = rope,ci = ci)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Proportion of samples inside the ROPE [-0.05, 0.05]:\n\nterm              | contrast | inside ROPE\n------------------------------------------\nexclamation_count |    dY/dX |     98.05 %\npc_aggression     |    dY/dX |    100.00 %\npc_incoherence    |    dY/dX |      0.00 %\nword_count        |    dY/dX |    100.00 %\n```\n\n\n:::\n:::\n\n\n\n\nRemember, we set our ROPE at ±5%—any effect smaller than a 5 percentage point change in spam probability is too small to matter for practical spam filtering. Now watch what happens:\n\nThree of our four \"statistically significant\" predictors fall completely within the ROPE:\n\n-   **pc_aggression**: 100% of the posterior is inside ROPE\n-   **word_count**: 100% of the posterior is inside ROPE\n-   **exclamation_count**: 98.05% of the posterior is inside ROPE\n\nOnly **pc_incoherence**, linguistic incoherence, stands apart with 0% inside the ROPE—this is our sole predictor with both statistical AND practical significance.\n\nThis is exactly why HDI+ROPE analysis is so powerful. Traditional significance testing would have given us four \"significant\" results, potentially leading to overengineered spam filters that track features with negligible real-world impact. But our Bayesian approach reveals the truth: only linguistic incoherence provides meaningful signal for spam detection.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-13-1.pdf){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\nThe visualization brings this distinction to life (note: I've omitted word_count from the plot for visual clarity). The gray distributions cluster tightly around zero, barely venturing beyond our ROPE boundaries despite technically excluding zero. These are our \"statistically significant but practically negligible\" effects. In stark contrast, the pc_incoherence distribution (in blue) boldly extends well beyond the ROPE, centered around -17% with most of its mass between -20% and -14%.\n\n##### Exploring Interactions\n\nSo far, we've examined how each feature independently affects spam probability. But real-world spam detection is more nuanced—the impact of one feature often depends on the context provided by others. With our model specification inspecting interactions, we're explicitly allowing for these interdependencies.\n\nWhen working with continuous predictors, interactions create a little twist: the effect of one variable literally changes as we move along the range of another variable. Think of it this way—the impact of aggressive language on spam probability might be minimal in very short messages (where there's little room for aggression to manifest) but could become substantial in longer messages (where sustained aggressive tone becomes more apparent).\n\nTo capture these shifting relationships, we need to examine how effects vary across different contexts. The `avg_slopes` function with `datagrid` helps us do exactly this—it calculates the average effect across specified points along the moderating variable's distribution. This gives us a single summary measure of how strong the interaction effect is overall.\n\nLet me demonstrate with our four interaction pairs:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Interaction 1: How does pc_incoherence's effect vary with message length?\nincoherence_by_wordcount <- avg_slopes(\n    spam_model,\n    variables = \"pc_incoherence\",\n    newdata   = datagrid(\n        word_count = quantile(model_data$word_count, \n                            probs = c(.05, .25, .50, .75, .95))\n    ),\n    type      = \"response\"\n)\n\n# Interaction 2: How pc_incoherence's effect changes with exclamation count\nincoherence_by_exclamation <- avg_slopes(\n    spam_model,\n    variables = \"pc_incoherence\",\n    newdata   = datagrid(\n        exclamation_count = quantile(model_data$exclamation_count, \n                                   probs = c(.05, .25, .50, .75, .95))\n    ),\n    type      = \"response\"\n)\n\n# Interaction 3: How pc_aggression's effect changes with exclamation count\naggression_by_exclamation <- avg_slopes(\n    spam_model,\n    variables = \"pc_aggression\",\n    newdata   = datagrid(\n        exclamation_count = quantile(model_data$exclamation_count, \n                                   probs = c(.05, .25, .50, .75, .95))\n    ),\n    type      = \"response\"\n)\n\n# Interaction 4: How pc_aggression's effect changes with word count\naggression_by_wordcount <- avg_slopes(\n    spam_model,\n    variables = \"pc_aggression\",\n    newdata   = datagrid(\n        word_count = quantile(model_data$word_count, \n                            probs = c(.05, .25, .50, .75, .95))\n    ),\n    type      = \"response\"\n)\n\nbind_rows(\nrope(incoherence_by_wordcount, range = rope, ci = ci) %>%\n  mutate(Parameter = \"incoherence × word_count\"),\n\nrope(incoherence_by_exclamation, range = rope, ci = ci) %>%\n  mutate(Parameter = \"incoherence × exclamation\"),\n\nrope(aggression_by_exclamation, range = rope, ci = ci) %>%\n  mutate(Parameter = \"aggression × exclamation\"),\n\nrope(aggression_by_wordcount, range = rope, ci = ci) %>%\n  mutate(Parameter = \"aggression × word_count\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Proportion of samples inside the ROPE [-0.05, 0.05]:\n\nParameter                 | inside ROPE\n---------------------------------------\nincoherence × word_count  |      0.00 %\nincoherence × exclamation |      0.00 %\naggression × exclamation  |     78.62 %\naggression × word_count   |    100.00 %\n```\n\n\n:::\n:::\n\n\n\n\nLooking at these interaction results, a clear pattern emerges. The two interactions involving linguistic incoherence both show substantial effects that completely escape our ROPE boundaries. This tells us that incoherent messages are strongly associated with legitimate (non-spam) classification, and this relationship holds steady whether we're looking at short or long messages, and whether they're punctuated heavily or not.\n\nThink about what this means in practical terms. When a message scores high on incoherence—perhaps it's a hastily typed personal message or an auto-generated notification with templated chunks—it's substantially less likely to be spam. This effect averages around -20% across different message lengths and punctuation patterns. The consistency is striking: whether it's a brief \"Running late, see u soon!\" or a longer rambling message full of typos and incomplete thoughts, linguistic incoherence signals authenticity rather than commercial intent.\n\nIn contrast, the aggression interactions tell a different story. The effect of aggressive language is almost entirely contained within our ROPE boundaries, especially when considered across different message lengths (100% in ROPE). Even the interaction with exclamation marks shows only marginal practical importance (78.62% in ROPE). This suggests that while aggressive language might technically interact with these features, the real-world impact is too small to matter for spam detection.\n\nHere's the visualization showing all four interaction effects:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Beyond!!!_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-align='center' width=80%}\n:::\n:::\n\n\n\n\n\n## Wrapping Up: A New Lens for Binary Classification\n\nWe started this journey frustrated with log-odds coefficients that nobody could interpret. Through the combination of Bayesian inference, marginal effects, and HDI+ROPE analysis, we've transformed an opaque logistic regression into a story anyone can understand.\n\nThe key insights from our spam detection analysis paint a surprisingly clear picture. Among all the features we examined—aggression scores, word counts, exclamation marks, and linguistic incoherence—only one emerged as practically meaningful: messages that sound incoherent are substantially less likely to be spam. This single feature drives a 15-20% reduction in spam probability, dwarfing all other effects. While this finding seems counterintuitive at first, it likely reflects how legitimate personal messages are often hastily typed and fragmented, while modern spam operations craft increasingly polished, coherent messages to evade filters. Such unexpected patterns highlight exactly why we need interpretable models—they reveal when reality doesn't match our assumptions.\n\nBut the real victory here isn't just about spam detection. It's about the analytical framework we've demonstrated. We tackled one of Bayesian analysis's most intimidating challenges—setting priors for log-odds coefficients—by developing a practical heuristic that translates familiar Cohen's d effect sizes into appropriate prior distributions. By combining `brms` for robust Bayesian estimation, `marginaleffects` for interpretable effect sizes, and `bayestestR` for principled decision-making, we've created a workflow that turns statistical significance into practical insight. No more explaining odds ratios to confused stakeholders. No more pretending that p < 0.05 means something matters in the real world.\n\nThe HDI+ROPE approach deserves special emphasis. It elegantly solves the fundamental tension in applied statistics: distinguishing between effects we can detect and effects that actually matter. In our analysis, we found multiple \"statistically significant\" predictors that were practically useless—a distinction that traditional methods would have missed entirely.\n\nFor practitioners working with binary outcomes—whether in spam detection, medical diagnosis, customer churn, or any other domain—this framework offers a path forward. Set your ROPE based on domain expertise, specify your priors using the Cohen's d translation trick, fit your model with confidence, and let the posterior distributions tell you not just what's real, but what's worth caring about. The result is statistical analysis that speaks the language of practical decision-making, turning the notorious complexity of logistic regression into insights that drive action.",
    "supporting": [
      "Beyond!!!_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}