---
title: "The Dot-Probe Task is Probably Fine"
subtitle: "Bayesian modeling of reliability with `brms`"
author: "Shachar Hochman"
date: last-modified
image: reliability_shift.gif
post-type: article
categories: 
  - bayesian
  - reliability 
  - HLM
format:
  html:
    toc: true
    code-fold: false
    fig-align: center
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  fig.align = "center",
  fig.width = 6, 
  fig.height = 4,
  out.width = "80%", 
  echo = TRUE,               # Show code by default
  warning = FALSE,           # Suppress warnings
  message = FALSE            # Suppress messages
)
options(digits = 3, width = 90)
```

## Why Are We Here?

-   **Goal:** Discuss two methods to detect reliability (of cognitive tasks in this case) using Bayesian hierarchical linear models (HLMs) in `brms`.

-   **Case Study:** The Dot-Probe Task. Contrary to a recent publication that claims the emotional dot-probe task is not a reliable measure, I'll show that more nuanced modeling suggests the task is probably "fine" reliability-wise.

-   **The Bottom Line:** This tutorial-like post demonstrates how Bayesian HLM can fundamentally change reliability assessment. By analyzing trial-level data rather than aggregated scores, these methods can uncover meaningful signal where traditional approaches see only noise---potentially rehabilitating numerous tasks previously deemed psychometrically inadequate.

::: callout-caution
## I make assumptions (too)!

I assume you:

-   are vaguely familiar with **hierarchical linear models (HLM)** and **Bayesian statistics** [^1].

-   know basic concepts in psychometrics - mainly the idea of "**reliability**".

This post uses R with a `tidyverse` approach. My goal is to show how others' great theoretical works can be applied in `brms`. If you're already a fan, welcome. If not, get ready to fall in love!
:::

[^1]: If you're wondering how to familiarize yourself with this material, I recommend starting with the excellent book ["A Student's Guide to Bayesian Statistics" by Ben Lambert](https://a.co/d/hd7utNs), which offers a fantastic introduction, especially for those with some understanding of frequentist statistics. Now, I'm a spiral learner---I often need to revisit concepts from different perspectives. With that in mind, I suggest following up with ["Statistical Rethinking"](https://a.co/d/hv38sCG) and/or ["Doing Bayesian Data Analysis"](https://a.co/d/6EG8AhU), alongside the legendary bookdowns by [Solomon Kurz](https://solomonkurz.netlify.app/book/) --- As the title of the callout probably reveals, I am a fan of his writings.

## A Blessing and a Curse - the "Reliability Paradox"

Scientists love finding significant results. Cognitive psychologists, in particular, aim to build "robust tasks"---ones that consistently produce similar significant effects when averaged across participants. When participants perform more similarly (showing less between-subject variance), effects tend to be more stable and therefore significant, leading to happy researchers. But here's the ironic twist: to get that consistent effect, tasks often get designed or refined until nearly all participants show the same pattern. In other words, cognitive psychologists (perhaps inadvertently) end up with tasks that minimize individual differences.

This is a thorny situation that reveals itself when researchers try to correlate their robust tasks performance with other measures of individual differences like questionnaires, or performance on other tasks. That's when they discover there simply isn't enough variability between participants to work with and to differentiate between their subjects.

The complexity worsens when the sought-after effect is a difference between two conditions of the same task (i.e., a within-subject condition). Take, for instance, when we measure attentional bias by comparing how quickly people respond to a probe that appears behind threatening versus neutral images---in this case researchers subtract one reaction time (RT; the score from the threatening stimuli) from another (the score from the neutral stimuli) in what's known as the dot-probe task. This introduces the "reliability of difference scores" problem into play---a well-documented challenge in measuring and understanding change. For both theoretical and statistical reasons, subtracting two within-subjects conditions produces individual difference scores with poor psychometric properties. These scores typically show weak correlations with themselves both when taken across time points (i.e., low test-retest reliability) and within a single administration (i.e., low internal reliability). And here is where the thorn hits again, because if these measures can't correlate with themselves, how (in the hell) can they correlate with anything else[?](https://tenor.com/en-GB/view/ru-paul-ru-pauls-drag-race-can-i-get-an-amen-amen-gif-16265919)

This tension between "robust tasks" (good for group-level effects) and high reliability (good for individual differences) is the heart of the so-called "reliability paradox," as popularized by [Hedge et al.](https://rdcu.be/d7pS5) and [Draheim et al.](https://doi.org/10.1037/bul0000192). These well-rounded and accessible papers explore this paradox in depth, making further elaboration here unnecessary. The key insight here is that, ***there is an inherent pitfall in the meeting point of the endeavor after robust tasks, difference scores and individual differences***.

## On repelling the reliability curse

There are generally two approaches to addressing the reliability paradox:

1.  **Better task design** -- creating more challenging tasks or developing multiple ways to tap into the construct of interest, thereby generating more within-subjects variation. This direction has already yielded promising results ([for example](https://rdcu.be/d7p8Y)).

2.  **Nuanced Statistical modeling** - analyzing trial-by-trial data with a model (often Bayesian HLM) that explicitly teases out and retains individual differences.

### On Bayesian Hierarchical Models

I won't re-argue the entire case for Bayesian HLMs here (see footnote 1, [these great materials](https://m-clark.github.io/posts/2019-05-14-shrinkage-in-mixed-models/), [and don't miss Nathaniel Haines blog](https://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error/)). Suffice it to say that:

-   <u>Trial-Level modeling:</u> Bayesian HLMs are generative models that we will feed trial-by-trial data. This means our models produce informed distributions for each participant in each condition. By considering trial-by-trial variation through this model-informed approach, we get nuanced measures of individual performance---precious information that would be lost in traditional ANOVA-style aggregation.

-   <u>Distributional Flexibility:</u> Traditional linear models or ANOVAs assume our measures ultimately follow a Gaussian distribution. With Bayesian HLMs, we can choose distributions that actually match our data. Take RTs, for instance---they're right-skewed and always positive. We can model this reality using a lognormal distribution (more on this later), allowing individual distributions to follow suit. This capability is particularly valuable for individual differences research.

-   <u>Posterior Distribution</u>: the Bayesian philosophy itself offers a distinct advantage. Instead of getting a single reliability value, we get posterior distributions---entire spectrums of possible values. This provides a richer, more nuanced perspective on reliability in our data.

## The Case Study: Revisiting the Dot-Probe Task

[Xu et al. (2024)](https://doi.org/10.1177/21677026241253826) have recently provided a compelling case study. They set out to test whether the "emotional dot‐probe task"---a long-standing paradigm in cognitive psychology---can truly capture an attentional bias to threat with any consistency. In a typical trial of the dot-probe task, two images (one threatening, one neutral) are flashed on opposite sides of the screen. After a brief interval, these images disappear and a target (often a letter like "E" or "F") appears in the location of one of the images. The idea is that if you're quicker to respond when the target replaces the threat, you've got a bias toward threat. Xu et al. took this classic design further by testing 36 variations of the task---tweaking everything from stimulus type (faces, scenes, snakes/spiders) to stimulus orientation (horizontal vs. vertical) and the timing of the stimulus onset asynchronies (SOAs; 100, 500, or 900 ms). Their stark finding: almost all variations of the task produced internal reliability estimates that were essentially zero, suggesting that this task holds little promise for detecting individual differences.

In this post, I reanalyze a subset of Xu et al.'s data with Bayesian HLM, hypothesizing that a more sophisticated approach might rescue some of that lost individual variance.

::: {.callout-tip collapse="true"}
#### Pre-processing the Data

I took the large datasets from Xu et al. and made several key decisions:

1.  Focusing on Study 1: This study closely examined the reliability of the emotional dot‐probe task, making it ideal for reanalysis.

2.  Selecting Task Variants: To ensure a substantial sample size, I combined data from six variants of the task (out of the 36 tested in the paper). All used faces as threatening stimuli on vertical display, differing in their SOAs and whether additional neutral trials were included. Only threat-congruent (TC) and threat-incongruent (TIC) trials were analyzed.

3.  <u>Data Cleaning and Filtering</u>: Building on Xu et al.'s exclusion criteria---participants with less than 60% accuracy or median RTs under 300 ms---I added a few more:

-   Included only mouse responses to reduce device-related variability.

-   Retained only correct response.

-   Excluded responses faster than 250 ms (likely anticipatory) and slower than 3500 ms (potential distractions).

-   Outliers control: I removed trials where RTs deviated more than 3.29 [median absolute deviations from the median](https://rdcu.be/d8upF) per subject and condition.

These are quite conservative steps. While many may not be strictly necessary for individual differences studies, they are standard in classic cognitive task analyses. Our total sample size now is 698 participants (!).

Here is the R code for the preprocessing:

```{r message=FALSE}
library(tidyverse)

`%ni%` <- Negate(`%in%`)

Study_1 <- readr::read_csv("openData_study1_trials_102823.csv")
s1.all.outcomes <- readr::read_csv("openData_study1_outcomes_102823.csv")

# Exclude participants with <60% accuracy or median RT <300 ms
s1.excluded.id <- s1.all.outcomes %>%
  filter(all_accuracy < 0.6 | all_medRTc < 300) %>%
  pull(id)


Study1_fs <- Study_1 %>%
  filter(id %ni% s1.excluded.id, 
         condition != "practice", 
         resp_type != "timeout") %>%
  filter(test_id %in% c(1:3, 19:21), 
         resp_type == "mouse") %>% 
  mutate(
    RT = as.numeric(rt) / 1000,  # Convert RT to seconds
    accuracy = as.numeric(correct),
    trial.all.type = row_number(), 
    split = as.factor(ifelse(trial.all.type %% 2 == 1, 1, 2)), 
    condition = case_when(
      condition == 1 ~ "TC",
      condition == 2 ~ "TIC",
      condition == 3 ~ "TT",
      condition == 4 ~ "NN"
    )
   ) %>%
   filter(RT > 0.25, RT < 3.5, accuracy == 1, condition %in% c("TC", "TIC")) %>%
   group_by(id, condition) %>%
   mutate(Outlier = c(datawizard::standardize(RT, robust = TRUE))) %>%
   ungroup() %>% 
   filter(abs(Outlier) < 3.29)

```
:::

# The Rouder-Haaf Model

[In their fantastic paper](https://rdcu.be/d8uJ4), Rouder and Haaf (2019) propose a Bayesian HLM that resembles a factorial ANOVA---but with a twist that lets us directly extract a reliability coefficient. For a split-half scenario (odd vs. even trials), the model is:

``` r
RT ~ split * condition + (split * condition | subject)
```

Which can also be written as:

``` r
RT ~ split + condition + split:condition +
  (split + condition + split:condition | subject)
```

The fixed‐effects part captures overall group trends, while the random‐effects part allows individual participants to deviate from these patterns. This setup is particularly handy for evaluating how each person's *condition* effect (threat‐congruent minus threat‐incongruent) remains stable or fluctuates across splits (e.g., odd vs even trials, or different sessions).

Rouder and Haaf conceptualize reliability as a proportion of variance -- essentially asking: *How much of the condition effect is preserved after we remove the fluctuations due to the split (or time) effect, relative to the total variability?* In their framework:

-   $\sigma_{\omega}^{2}$ is the variance of the random slope for `condition|subject`, reflecting the stable, overall individual differences in the condition effect.

-   $\sigma_{\gamma}^{2}$ is the variance of the random slope for `split:condition|subject`, capturing how much that condition effect fluctuates from one split (or session) to the next.

They then define reliability as:

$$
\rho = \frac{\sigma_{\omega}^{2} - \sigma_{\gamma}^{2}}{\sigma_{\omega}^{2} + \sigma_{\gamma}^{2}}.
$$

In plain language, the total variance in the condition effect can be thought of as arising from two sources: the consistent part ($\sigma_{\omega}^{2}$) represents the enduring condition effect, while the fluctuating part ($\sigma_{\gamma}^{2}$) indicates how much that effect varies over time. The reliability estimate ($\rho$) is essentially the fraction of the overall condition effect that remains stable after removing the variability due to time. In other words, if most of the variance is consistent, reliability is high; if a large portion is unstable, reliability is low.

## Rouder-Haaf Model in `brms`

Now that we understand the conceptual framework, let's see how to implement this in `brms`. The model specification needs to be careful and precise - we're dealing with reaction time data that has specific characteristics, and we want to capture best the nature of our measurements:

``` r
library(brms)
library(bayestestR)

options(contrasts = c("contr.equalprior", "contr.poly"))

priors <- c(
  prior(exponential(1), class = "sd", group = "id"),
  prior(exponential(1), class = "sd", group = "id", dpar = "sigma"),  
  
  
  prior(normal(0, 1), class = "b", dpar = "sigma"),  
  
  # Fixed effects
  prior(normal(0, 0.1), class = "b"))
```

We start by loading the necessary packages and setting proper sum to zero contrast coding. The `contr.equalprior` ensures unbiased Bayesian estimation when working with factors - unlike traditional effect or treatment coding which can lead to biased priors ([see here for a detailed explanation](https://easystats.github.io/bayestestR/articles/bayes_factors.html#contr_bayes)).

Next, we define relatively weakly informative priors for all parameters. The random-effects standard deviations (for both the mean and the residual variance) receive `exponential(1)`, reflecting mild assumptions about individual variability. Meanwhile, the fixed effects for the mean RT are assigned a `normal(0, 0.1)` prior, implying modest expected effect sizes. Finally, we allow for some uncertainty in the residual standard deviation by placing a `normal(0, 1)` prior on any parameters governing sigma. These choices are conservative enough to encourage stable estimation, yet flexible enough to capture meaningful individual differences in response times.

``` r
DotProbeModel.RouderHaafModel.RDS <- brm(
  formula = bf(
    RT | trunc(lb = 0.25, ub = 3.5) ~ 
      condition * split +
      (condition * split|p|id),
    
    sigma ~ 
      condition * split +
      (condition * split|p|id)
    ),
  prior = priors,

  family = lognormal(), data = Study1_fs,

  iter = 4000, warmup = 2000,

  chains = 4,  cores = 4,

  threads = threading(2),

  control = list(adapt_delta = 0.95,
                 max_treedepth=12),

  init = 0, backend = "cmdstanr")
```

The model formula contains several important features:

1.  <u>Truncation:</u> `trunc(lb = 0.25, ub = 3.5)` tells the model our response variable (RT) cannot fall below 0.25 seconds or exceed 3.5 seconds. This matches our preprocessing decisions and helps the model make more accurate predictions by respecting the actual bounds of our data.

2.  <u>Distribution Choice:</u> `family = lognormal()` is crucial. Reaction times follow a characteristic right-skewed distribution---they can't be negative, tend to have a longer right tail, and the variability often increases with the mean. Importantly, the lognormal distribution captures these properties naturally by modeling RTs on the log scale, where effects are multiplicative (e.g., a 10% slowdown instead of ±X ms).

    This multiplicative framework elegantly handles proportional relationships (e.g., a participant slowing by 10% in threat trials retains the same effect size whether their baseline is 500 ms or 1000 ms). In the same vein, reliability estimates derived from the log scale reflect proportional differences. Nevertheless, it is mostly common to report and use absolute differences in raw milliseconds. Later, we'll reconcile this by converting our reliability estimates to the raw RT scale using posterior predictions---ensuring consistency with Xu et al.'s approach and broader psychometric conventions.

3.  <u>The Sigma:</u> The `sigma ~ condition * split + (condition * split | p | id)` part of the formula recognizes that people aren't just different in their overall consistency, but that their variability can also differ by condition and split. Imagine some participants who remain steady across all conditions, while others might fluctuate more in one condition than another. By letting each participant have their own "baseline consistency" as well as condition- and split-specific effects, we are not just acknowledging that people vary overall---we're allowing these differences to manifest across different experimental factors, rather than forcing everyone into a single error structure.

    The lognormal distribution already accounts for the fact that reaction times naturally "spread out" as they lengthen (e.g., a 3-second average often has bigger ups/downs than a 1-second average). Here, we go a step further: each person can have a distinct sigma profile for each combination of condition and split---capturing a richer picture of individual differences.

4.  <u>Correlated Individual Effects:</u> The `|p|` syntax in both the main formula `(condition * split |p|id)` and the sigma formula`(condition * split |p|id)` allows the model to account for an important reality of reaction time data: participants with longer RTs typically also show larger variability in their responses. Instead of assuming independent effects, this structure lets the model estimate *correlations* between individual differences in mean RTs and response variability. By doing so, it refines individual estimates even further.

### Extracting the reliability

Just before examining the reliability using a full Bayesian approach à la "Rouder and Haaf", let's look at what we get using the classic method for calculating internal consistency.

```{r echo = FALSE, warning=FALSE, message= FALSE}
library(bayestestR)

# First create the data frame with odd/even terminology
condition_differences <- Study1_fs %>%
  # Add the half.all.type column
  mutate(half.all.type = ifelse(trial.all.type %% 2 == 1, "Odd", "Even")) %>%
  # Group by id, half.all.type, and condition to get means
  group_by(id, half.all.type, condition) %>%
  summarize(mean_RT = mean(RT, na.rm = TRUE)) %>%
  # Pivot wider to get conditions in separate columns
  pivot_wider(
    names_from = condition,
    values_from = mean_RT
  ) %>%
  # Calculate the difference (TIC - TC)
  mutate(condition_difference = TIC - TC) %>%
  # Select only the columns we need
  select(id, half.all.type, condition_difference) %>%
  # Pivot wider to get odd/even in separate columns
  pivot_wider(
    names_from = half.all.type,
    values_from = condition_difference
  )

# Create the plot
# Calculate correlation and Spearman-Brown reliability
cor_coef <- cor(condition_differences$Odd, condition_differences$Even)
spearman_brown <- (2 * cor_coef) / (1 + cor_coef)

library(ggplot2)

ggplot(condition_differences, aes(x = Odd, y = Even)) +
  geom_point(alpha = 0.5, color = "navy") +
  geom_smooth(method = "lm", color = "red") +
  theme_minimal() +
  labs(
    title = "Internal Consistency - Classic Aggregation Style",
    subtitle = bquote(
      bold("Pearson correlation (r) = ") ~ .(round(cor_coef, 3)) * "\n" * "|" *
      "\n" * bold("Spearman-Brown ρ = ") ~ .(round(spearman_brown, 3))
    ),
    x = "Odd Trials (TIC - TC)",
    y = "Even Trials (TIC - TC)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    plot.subtitle = element_text(hjust = 0.5, size = 12, face = "bold", color = "darkred"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )

```

The results mirror those reported by Xu et al., showing a nearly zero correlation of -0.019 (and similarly poor results after applying the Spearman-Brown correction) -- in fact, slightly negative in this case.

**Now for the main course: Rouder and Haaf reliability estimate.** Using `tidybayes`, I extract the standard deviations of the random effects for both the congruity effect and the interaction and transforming them to the response scale.

``` r
Var.df <- DotProbeModel1 %>%
  spread_rvars(sd_id__trial_type1, `sd_id__trial_type1:half.all.type1`)
  
Var.df <- Var.df %>% 
  mutate(
  sd_response_condition1_response = exp(sd_id__condition1^2 / 2) * sqrt(exp(sd_id__condition1^2) - 1),
  sd_response_interaction_response = exp(`sd_id__condition1:split1`^2 / 2) * sqrt(exp(`sd_id__condition1:split1`^2) - 1)
  )
```

I square the standard deviations in order to get variance,

``` r
Var.df_variance <- Var.df %>%
  mutate(across(everything(), ~ .^2)) 
```

and then what I have left is to perform the subtraction and division (the rvars structure makes this remarkably straightforward):

``` r
Rouder.Haaf.numerator <- 
  Var.df_variance$sd_id__trial_type1 - 
  Var.df_variance$`sd_id__trial_type1:half.all.type1`

Rouder.Haaf.denominator <- 
  Var.df_variance$sd_id__trial_type1 + 
  Var.df_variance$`sd_id__trial_type1:half.all.type1`

Rouder.Haaf.Reliability <- Rouder.Haaf.numerator/Rouder.Haaf.denominator
```

```{r echo=FALSE, warning=FALSE, message= FALSE}
library(bayestestR)
library(posterior)
library(tidybayes)
library(ggdist)
library(brms)

DotProbeModel.RouderHaafModel <- readRDS("DotProbeModel.RouderHaafModel.RDS")

Var.df <- DotProbeModel.RouderHaafModel %>%
  spread_rvars(sd_id__condition1, `sd_id__condition1:split1`) 


Var.df <- Var.df %>% 
     mutate(
         sd_response_condition1_response = exp(sd_id__condition1^2 / 2) * sqrt(exp(sd_id__condition1^2) - 1),
         sd_response_interaction_response = exp(`sd_id__condition1:split1`^2 / 2) * sqrt(exp(`sd_id__condition1:split1`^2) - 1)
     )

Var.df_variance <- Var.df %>%
  mutate(across(everything(), ~ .^2)) 


Rouder.Haaf.numerator <- 
  Var.df_variance$sd_response_condition1_response - 
  Var.df_variance$sd_response_interaction_response

Rouder.Haaf.denominator <- 
  Var.df_variance$sd_response_condition1_response + 
  Var.df_variance$sd_response_interaction_response

Rouder.Haaf.Reliability <- Rouder.Haaf.numerator/Rouder.Haaf.denominator


# Compute the median reliability value
median_val <- median(Rouder.Haaf.Reliability)

tibble(reliability = Rouder.Haaf.Reliability) %>%
    ggplot(aes(xdist = reliability)) +
    # Draw the slab with automatic shading via fill_ramp
    stat_slab(
        aes(fill_ramp = after_stat(level)),
        fill = "#adecce",
        .width = c(0.5, 0.8, 0.9, 0.95, 0.99),
        point_interval = median_hdci,    # Use the HDI rather than the default quantile interval
        slab_color = "grey30",
        slab_size = 0.75
    ) + scale_fill_ramp_discrete(aesthetics = "fill_ramp")+
    # Remove legends for fill and fill_ramp
    guides(fill = "none", fill_ramp = "none") +
    # Add a spike for the median (dashed) with a lila color
    stat_spike(
        aes(linetype = "Median", color = "Median"),
        at = c(median),
        show.legend = TRUE
    ) +
    # Add a spike for the 95% HDI (solid) with black color
    stat_spike(
        aes(linetype = "95% HDI", color = "95% HDI"),
        at = function(x) hdci(x, .width = 0.95),
        show.legend = TRUE
    ) +
    # Share the thickness scale between the slab and spikes
    scale_thickness_shared() +
    # Define the legend for the summary statistic spikes
    scale_linetype_manual(
        name = "Summary Statistic",
        values = c("Median" = "longdash", "95% HDI" = "solid")
    ) +
    scale_color_manual(
        name = "Summary Statistic",
        values = c("Median" = "#8494FF", "95% HDI" = "#000000")
    ) +
    labs(
        title = "Rouder-Haaf Reliability Distribution",
        subtitle = "Credible intervals and median of reliability coefficients",
        x = "Reliability Coefficient",
        caption = "Dashed line: median; Solid line: 95% Highest Density Interval (HDI).\nFill shades represent the 50%, 80%, 90%, 95% and 99% credible intervals."
    ) +
    theme_minimal() +
theme(
    plot.title    = element_text(face = "bold", size = 16, margin = margin(b = 10), hjust = 0.5),
    plot.subtitle = element_text(color = "grey40", size = 12, margin = margin(b = 20), hjust = 0.5),
    axis.title.x  = element_text(size = 12, margin = margin(t = 10)),
    axis.title.y  = element_blank(),
    axis.text.y = element_blank(),
    axis.text     = element_text(size = 11, color = "grey30"),
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank(),
    plot.caption  = element_text(hjust = 0.5),
    legend.key         = element_rect(fill = "white", color = "black"),
    legend.box.background = element_rect(color = "grey30", size = 0.25)
)+
    # Add a label for the median value, nudged slightly upward so it's visible
    geom_label(
        data = tibble(median_val = median_val),
        mapping = aes(x = median_val, y = 0, label = sprintf("%.2f", median_val)),
        inherit.aes = FALSE,
        color = "black",
        fill = "white",
        size = 3
    )
```

This is the posterior reliability coefficient distribution---a distribution of possible values for the reliability coefficient we are estimating, where the density reflects how likely each value is.

What emerges here, is in my opinion, a fascinating picture of informative Bayesian complexity:

-   First, we see the remarkable benefits of the Bayesian HLM approach, where `r round(100 * posterior::Pr(Rouder.Haaf.Reliability > -0.019), 2)`% of the possible reliability values exceed the raw-aggregated value of -0.019. This dramatic difference highlights how traditional aggregation methods might severely underestimate the task's reliability.

-   At the same time, the actual reliability of the Dot-probe task isn't definitively established. The 95% credible interval suggests reliability values range from `r round(bayestestR::hdi(Rouder.Haaf.Reliability)[[3]], 2)` to a practically perfect `r round(bayestestR::hdi(Rouder.Haaf.Reliability)[[4]], 4)`. While this wide posterior distribution indicates considerable uncertainty about the task's reliability, it also allows us to make informed probability statements about specific thresholds. Based on the posterior distribution, there's a `r round(100 * posterior::Pr(Rouder.Haaf.Reliability > 0.60), 2)`% chance that the Dot-probe task reliability exceeds 0.6, a `r round(100 * posterior::Pr(Rouder.Haaf.Reliability > 0.70), 2)`% chance it exceeds 0.7, and a `r round(100 * posterior::Pr(Rouder.Haaf.Reliability > 0.80), 2)`% chance it exceeds 0.8. Such nuanced probabilistic conclusions aren't possible with classic models and give researchers the tools to make reasoned decisions.

-   But what's the most likely reliability? The Maximum A Posteriori probability estimate (MAP) point estimates follows for the most likely value and in this case indicates on practically perfect reliability of `r round(bayestestR::map_estimate(Rouder.Haaf.Reliability)[[2]], 4)`. More conservative estimates include the median at `r round(median(Rouder.Haaf.Reliability), 4)` and the mean at `r round(mean(Rouder.Haaf.Reliability), 4)`. All these point estimates suggest the task is substantially more reliable than previously thought, with even the most conservative estimate showing strong reliability.

Reminder: our example deals with "split-half" reliability, a type of "internal consistency". It typically exceeds what you'd get with, for example, a test--retest scenario (different sessions, potential fatigue effects, etc.), so you might consider the values here as an upper bound on broader "reliability".

# The Chen et al. Model

[Chen et al.](https://doi.org/10.1016/j.neuroimage.2021.118647) propose a complementary approach, and focusing on the within-subject trial-level variance. In doing so, they show that ignoring this cross‐trial variance (sometimes referred to as $\sigma_0$) can lead to systematically underestimated reliability, especially for difference scores.

### Why Model Cross‐Trial Variance?

In many classic cognitive tasks (the dot‐probe included), we observe two major sources of variability:

1.  **Between‐subject**: Some participants consistently differ in how they respond to threat vs neutral stimuli by having larger or smaller effects.
2.  **Within‐subject**: Each participant's reaction times naturally fluctuate across trials.

The conventional aggregating approach assume this "within-subject" variance is constant across subject, and by doing so lumps all of a subject's trials for each condition into a single average, losing any sense of how variable the trials are. Chen et al. call this out as a missed opportunity: $\sigma_0$ is substantial, and a simple average can mask real individual differences, causing reliability measures to drop. In fact, they show that if cross‐trial variance is large relative to cross‐subject variance, a condition‐level analysis can severely underestimate reliability. Bayesian HLM with trial-level data can account for this variance. This provides more precise reliability estimates by incorporating and correctly partitioning that within‐subject trial‐to‐trial noise, rather than discarding it.

## Implementing Chen et al. in `brms`

Chen et al. take a distinct modeling approach by focusing on how condition effects (TC vs. TIC) vary across splits (odd vs. even trials). Their model omits both the global intercept and main effect of condition, instead modeling how these condition differences manifest through interactions with splits. This parameterization directly captures how stable individual differences in condition effects are across different portions of the task. Here's how we implement this approach in `brms`. Rather than using a standard factorial structure (`condition + split + condition:split`), we separate the random effects into distinct components for splits and condition-by-split interactions:

``` r
DotProbeModel.ChenModel <- brm(
  formula = bf(
    RT | trunc(lb = 0.25, ub = 3.5) ~ 0 +
      split+
      condition:split+
      (0+split|p|id)+
      (0+split:condition|p|id),
    
    sigma ~ 0 +
      split+
      condition:split+
      (0+split|p|id)+
      (0+split:condition|p|id)
  ),
  prior = priors,

  family = lognormal(), data = Study1_fs,

  iter = 4000, warmup = 2000,

  chains = 4,  cores = 4,

  threads = threading(2),

  control = list(adapt_delta = 0.95,
                 max_treedepth=12),

  init = 0, backend = "cmdstanr")
```

### Extracting the Reliability

The Chen et al. approach gives us a reliability estimate by correlating threat bias scores (TIC-TC) between splits. The process mirrors what we typically do in reliability analysis: we compute difference scores for each split level (odd vs even trials) and correlate them, but with an important Bayesian twist - we do this for every posterior draw from our model, giving us a full distribution of reliability estimates instead of just one number. I built on their approach by adding the Spearman-Brown correction - a standard adjustment in psychometrics that accounts for split-half calculations. We need this correction because we're essentially working with half the data in each split, and it makes our estimates more comparable to full-test reliability. Plus, since Xu et al. used this correction in their original analysis, it lets us make direct comparisons.

``` r

# Simulate expected RTs (milliseconds) for all splits/conditions
newdata <- tidyr::expand_grid(
  id = unique(Study1_fs$id),        
  split = c("1", "2"),         
  condition = c("TIC", "TC")          
)

pred <- add_epred_rvars(
  DotProbeModel.ChenModel,
  newdata = newdata,
  re_formula = NULL           
) %>% 
  mutate(condition = interaction(split, condition, sep = "_")) 

# Compute threat bias (TIC - TC) for each split
diff_scores <- pred %>%
  select(id, condition, .epred) %>%
  pivot_wider(names_from = condition, values_from = .epred) %>%
  mutate(
    Effect_1 = `1_TIC` - `1_TC`,  # Split 1 effect (ms)
    Effect_2 = `2_TIC` - `2_TC`   # Split 2 effect (ms)
  ) 

# Calculate reliability per posterior draw
cor_results <- 
  data.frame(
    correlation =  
      with(diff_scores, {
        rdo(
          cor(Effect_1, Effect_2)
          )})) %>%
  mutate(
    spearman_brown = (2 * correlation) / (1 + correlation)
    )
```

```{r echo=FALSE, warning=FALSE, message= FALSE}


DotProbeModel.ChenModel <- readRDS("DotProbeModel.ChenModel.RDS")

newdata <- tidyr::expand_grid(
  id = unique(Study1_fs$id),        
  split = c("1", "2"),         
  condition = c("TIC", "TC")          
)

pred <- add_epred_rvars(
  DotProbeModel.ChenModel,
  newdata = newdata,
  re_formula = NULL           
) %>% 
  mutate(condition = interaction(split, condition, sep = "_")) 

diff_scores <- pred %>%
  select(id, condition, .epred) %>%
  pivot_wider(names_from = condition, values_from = .epred) %>%
  mutate(
    Effect_1 = `1_TIC` - `1_TC`,  # Split 1 effect (ms)
    Effect_2 = `2_TIC` - `2_TC`   # Split 2 effect (ms)
  ) 

cor_results <- 
  data.frame(
    correlation =  
      with(diff_scores, {
        rdo(
          cor(Effect_1, Effect_2)
          )})) %>%
  mutate(
    spearman_brown = (2 * correlation) / (1 + correlation)
    )



#------------------------------------------#
# 7) Plot
#------------------------------------------#

post_corrs_long <- cor_results %>%
  pivot_longer(
    cols      = c(correlation, spearman_brown),
    names_to  = "cor_type",
    values_to = "cor_value"
  )


# Create the data frame using your method:
df <- post_corrs_long %>% 
  group_by(cor_type) %>% 
  summarize(x = rvar(cor_value))

# Compute median values (the rvar supports summary functions)
median_values <- df %>% 
  group_by(cor_type) %>% 
  summarize(median_val = median(x))

# Build the plot
p <- ggplot(df, aes(xdist = x, y = cor_type, fill = cor_type)) +
  stat_slab(
    aes(fill_ramp = after_stat(level)),
    .width = c(0.5, 0.8, 0.9, 0.95, 0.99),
    point_interval = median_hdci,    # Use the HDI rather than the default quantile interval
    slab_color = "grey30",
    slab_size = 0.75
  ) +
  scale_fill_manual(values = c("#E64B35B2", "#4DBBD5B2")) +
  guides(fill_ramp = "none", fill = "none") +
  # Spike for the median (dotted line)
  stat_spike(
    aes(linetype = "Median", color = "Median"),
    at = c(median),
    show.legend = TRUE
  ) +
  # Spike for the 95% HDI (solid line)
  stat_spike(
    aes(linetype = "95% HDI", color = "95% HDI"),
    at = function(x) hdci(x, .width = 0.95),
    show.legend = TRUE
  ) +
  # Spike for the MAP estimate is kept for future use, but commented out:
  # stat_spike(
  #   aes(linetype = "MAP estimate", color = "MAP estimate"),
  #   at = function(x) bayestestR::map_estimate(x)[[2]],
  #   show.legend = TRUE
  # ) +
  scale_thickness_shared() +
  # Manual scales for the spike lines (using black for both)
  scale_linetype_manual(
    name = "Summary Statistic",
    values = c("Median" = "longdash", "95% HDI" = "solid")
  ) +
  scale_color_manual(
    name = "Summary Statistic",
    values = c("Median" = "#8494FF", "95% HDI" = "#000000")
  ) +
  labs(
    title = "Split-Half Reliability Estimates",
    subtitle = "Comparing Pearson correlation and Spearman-Brown corrected reliability",
    x = "Reliability Coefficient",
    y = "",
            caption = "Dashed line: median; Solid line: 95% Highest Density Interval (HDI).\nFill shades represent the 50%, 80%, 90%, 95% and 99% credible intervals."
  ) +
  scale_y_discrete(labels = c("Pearson\nCorrelation", "Spearman-Brown\nCorrected")) +
  theme_minimal() +
theme(
  plot.title    = element_text(face = "bold", size = 16, margin = margin(b = 10), hjust = 0.5),
  plot.subtitle = element_text(color = "grey40", size = 12, margin = margin(b = 20), hjust = 0.5),
  axis.title.x  = element_text(size = 12, margin = margin(t = 10)),
  axis.text     = element_text(size = 11, color = "grey30"),
  axis.text.y   = element_text(hjust = 0.5), # This centers the y-axis text
  panel.grid.minor = element_blank(),
  panel.grid.major.y = element_blank(),
  plot.caption  = element_text(hjust = 0.5)
)+
  xlim(0, 1)

# Add geom_label for the median values.
(p + geom_label(
  data = median_values,
  mapping = aes(x = median_val, y = cor_type, label = sprintf("%.2f", median_val)),
  inherit.aes = FALSE,  # Do not inherit the global aesthetics (including xdist)
  color = "black",
  fill = "white",
  size = 3
))

```

The posterior distributions reveal 95% credible intervals ranging from `r round(bayestestR::hdi(cor_results$correlation)[[3]],digits = 2)` to `r round(bayestestR::hdi(cor_results$correlation)[[4]],digits = 2)` for the raw correlation, and from `r round(bayestestR::hdi(cor_results$spearman_brown)[[3]],digits = 2)` to `r round(bayestestR::hdi(cor_results$spearman_brown)[[4]],digits = 2)` for the Spearman-Brown corrected reliability, with median estimates of `r round(median(cor_results$correlation), digits = 2)` and `r round(median(cor_results$spearman_brown), digits = 2)`, respectively. These values suggest moderate to strong reliability, though somewhat lower than the Rouder-Haaf estimates. This difference is instructive -- while both approaches support the task's reliability, they do so through different lenses. The Chen et al. method provides more conservative estimates that may feel more familiar to researchers used to traditional reliability metrics, while still demonstrating substantially higher reliability than previously reported using simpler methods.

# What has changed?

In the first plot here, I presented raw, non-modeled data showing a near-zero correlation---a bleak outlook for the dot-probe task. But as we wrap up this journey, let's visualize the shift. The Bayesian models reveal a striking twist: what seemed like irredeemable noise in the aggregated scores transforms into a meaningful signal when analyzed trial-by-trial.

```{r echo = FALSE, warning=FALSE, message=FALSE}

library(gganimate)

# Prepare combined data (unchanged)
posterior_wide_corrected <- diff_scores %>%
  mutate(post_even = Effect_1,
         post_odd = Effect_2,
         id = as.numeric(as.character(id)))

combined <- condition_differences %>%
  mutate(
    id = as.numeric(as.character(id)),
    raw_even = Even,
    raw_odd = Odd
  ) %>%
  left_join(
    posterior_wide_corrected %>%
      group_by(id) %>%
      summarise(post_even = median(post_even),
                post_odd = median(post_odd)),
    by = "id"
  ) %>% 
  ungroup() %>%
  mutate(
    shift_even = post_even - raw_even,
    shift_odd = post_odd - raw_odd,
    raw_avg = (raw_even + raw_odd) / 2,
    post_avg = (post_even + post_odd) / 2,
    raw_rank = min_rank(desc(raw_avg)),
    post_rank = min_rank(desc(post_avg)),
    shift_distance = sqrt((post_even - raw_even)^2 + (post_odd - raw_odd)^2),
    rank_diff = post_rank - raw_rank,
    abs_rank_diff = abs(rank_diff)
  )

# Find subject with maximum rank change
id_most_moved <- combined %>%
  slice_max(abs_rank_diff, n = 1) %>% 
  pull(id)

# Create long format for animation
combined_long <- combined %>%
  pivot_longer(
    cols = c(raw_even, raw_odd, post_even, post_odd),
    names_to = c("state", "trial_type"),
    names_sep = "_"
  ) %>%
  pivot_wider(
    names_from = trial_type,
    values_from = value
  )

# State labels
state_labels <- c(
  raw = "Raw-Aggregated Estimates",
  post = "Bayesian Model Estimates"
)

# Color setup
color_palette <- c("#e6ecf5", "#6A5ACD", "#FF7609")
color_values <- scales::rescale(c(
  min(combined$shift_distance), 
  median(combined$shift_distance), 
  max(combined$shift_distance)
))

# Create animated plot
# animated_plot <- ggplot(combined_long, aes(x = even, y = odd)) +
#     geom_point(aes(color = shift_distance), alpha = 0.4, size = 3) +
#     geom_vline(xintercept = 0, color = "gray30") +
#     geom_hline(yintercept = 0, color = "gray30") +
#     geom_smooth(method = "lm", se = FALSE, color = "red") +
#     scale_color_gradientn(
#         colors = color_palette,
#         values = color_values,
#         name = "Shift Distance\n(raw → posterior)",
#         breaks = c(0, max(combined$shift_distance)),
#         labels = c("Small", "Large")
#     ) +
#     scale_x_continuous(
#         breaks = scales::extended_breaks(n = 8),
#         labels = scales::number_format(accuracy = 0.1)
#     ) +
#     scale_y_continuous(
#         breaks = scales::extended_breaks(n = 8),
#         labels = scales::number_format(accuracy = 0.1)
#     ) +
#     labs(
#         title = "Internal Consistency: {state_labels[closest_state]}",
#         x = "Even Trials (TIC - TC)",
#         y = "Odd Trials (TIC - TC)"
#     ) +
#     theme_minimal() +
#     theme(
#         plot.title = element_text(hjust = 0.5, size = 20, face = "bold"),
#         axis.title = element_text(size = 18),
#         axis.text = element_text(size = 12),  # Slightly larger text
#         axis.ticks = element_line(color = "gray30"),
#         axis.ticks.length = unit(4, "pt"),  # Longer ticks
#         panel.grid.major = element_line(color = "gray90", linewidth = 0.3),
#         panel.grid.minor = element_line(color = "gray95", linewidth = 0.1),  # Added minor grid
#         legend.position = "bottom"
#     ) +
#     transition_states(
#         state,
#         transition_length = 1,
#         state_length = 2
#     ) +
#     enter_fade() +
#     exit_shrink() +
#     ease_aes("cubic-in-out")

# # Render with higher resolution
# anim <- animate(
#     animated_plot,
#     width = 800,
#     height = 600,
#     fps = 10,
#     duration = 10,
#     renderer = gifski_renderer()
# )
# 
# # Save to file
# anim_save("reliability_shift.gif", animation = anim)


```

![](reliability_shift.gif){width="17.3cm"}

The animation's **gradient (blue → red)** maps this shift: cooler hues reflect stable participants, where raw scores align closely with model-adjusted estimates, suggesting reliable data. Redder points, however, mark *noisy individuals*---here, the model significantly adjusts their scores, pulling them toward group-level patterns. This divergence highlights why **trial-level modeling** succeeds where aggregation fails: by accounting for variability within each participant's trials, the model separates fleeting noise (e.g., momentary lapses) from stable bias scores, transforming a scattered cloud into a coherent signal.

# Summary

This analysis reveals how sophisticated statistical modeling can uncover reliability where simpler methods suggest none exists. Using two complementary Bayesian hierarchical approaches -- the Rouder-Haaf variance decomposition and the Chen et al. correlation-based method -- we find that the dot-probe task demonstrates moderate to strong reliability when analyzed appropriately, though with considerable uncertainty in these estimates.

Three key questions naturally emerge:

1.  **Which method is better?**

    I don't know, a good simulation study comparing these methods will have to answer that. From a practical side, both approaches have distinct merits: Rouder-Haaf builds on the common maximal random structure that cognitive researchers are familiar with, allowing extraction of both group (that is, ANOVA-like analyses) and reliable individual differences insights from a single model. Chen et al.'s approach provides the familiar framework of difference scores and Pearson correlations (which can be corrected). At this point, the choice may ultimately depend on specific research needs.

2.  **If the task effects are reliable indeed (at least "probably fine"), could one use its raw difference scores?**

    Unfortunately, no. While we've shown the task can be reliable, this reliability is tied specifically to our modeling approach. Remember - reliability is a property of the scores we calculate, not just the task itself. So if we find reliability using these models, we need to stick with model-based scores.

3.  **What's next for measurement?**

    Earlier, I presented two paths for addressing the reliability paradox: redesigning tasks or using sophisticated statistics. The results here suggest these approaches might be complementary rather than alternative solutions, an iterative process if you will. Indeed, while our models reveal meaningful reliability, the wide credible intervals suggest room for improvement. Combining better measurement practices (like adequate trial numbers) with sophisticated modeling could help us better separate true individual differences from trial-level noise. Put simply, the dot-probe task appears fundamentally sound - it might just need more trials to reach its full potential.

##### 

::: {.callout-note collapse="true" appearance="minimal"}
##### *Session Info*

```{r}
sessioninfo::session_info()
```
:::
